{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "\n<br>\n<font>\n<div dir=ltr align=center>\n<font color=0F5298 size=7>\n    System 2 - Homework 2<br>\n<font color=2565AE size=5>\n    Spring 2025<br>\n<font color=3C99D size=5>\n    Inference-time Scaling <br>\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full Name: Danial Parnian\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Assignment Overview (Part 1)\n\nIn this assignment, you will explore inference scaling techniques in large language models (LLMs) and evaluate their performance using the Math Benchmark. Throughout the notebook, you will learn about several inference methods, including:\n\n- **Chain-of-Thought (CoT):** A method where the model generates intermediate reasoning steps before providing the final answer.\n- **Best-of-n Sampling:** An approach that generates multiple candidate responses and selects the best one based on a scoring function.\n- **Beam Search:** A technique that expands several possible sequences simultaneously, choosing the most promising ones based on probability.\n- **Self-Refinement:** An iterative process where the model revises its output to improve accuracy and coherence.\n\nThe **Math Benchmark** is a suite of challenging mathematical problems designed to test the reasoning and problem-solving capabilities of LLMs. The benchmark includes a variety of questions ranging from basic arithmetic and algebra to more advanced topics such as geometry and calculus. For example, you might be asked to solve an equation like `2x + 5 = 15` or compute the derivative of a function, tasks that assess the model's ability to handle both straightforward and complex mathematical queries.\n\nBy the end of this assignment, you will have:\n- Gained a deeper understanding of inference time scaling methods in LLMs.\n- Compared the effectiveness of different inference techniques using a rigorous math evaluation framework.\n\nLet's dive into the notebook and begin exploring how these methods perform on a challenging set of math problems!\n",
   "metadata": {
    "id": "4yKp1NLIIia1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# installing Dependencies",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install vllm\n!pip install transformers accelerate datasets\n\nfrom IPython.display import clear_output\nclear_output()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T08:47:32.919943Z",
     "iopub.execute_input": "2025-05-23T08:47:32.920222Z",
     "iopub.status.idle": "2025-05-23T08:49:38.230826Z",
     "shell.execute_reply.started": "2025-05-23T08:47:32.920199Z",
     "shell.execute_reply": "2025-05-23T08:49:38.229740Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "* You should use this cell if you're running the notebook on Google Colab. If you're using Kaggle, you don't need to run this cell.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# !pip install --upgrade numpy\n# import os\n# os.kill(os.getpid(), 9)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T08:49:38.232854Z",
     "iopub.execute_input": "2025-05-23T08:49:38.233209Z",
     "iopub.status.idle": "2025-05-23T08:49:38.238554Z",
     "shell.execute_reply.started": "2025-05-23T08:49:38.233170Z",
     "shell.execute_reply": "2025-05-23T08:49:38.237799Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": "## vLLM: Accelerated Inference Engine for LLMs\n\nvLLM is an open-source project designed to optimize the loading and inference of large language models. By leveraging advanced memory management techniques and dynamic batching, vLLM significantly speeds up the inference process, making it easier to deploy and experiment with LLMs even on hardware with limited resources\nSo we use vLLM to get results faster.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## VLLM Server Setup and Initialization\n\nIn this section, we install the required packages, ensure that only one server instance is running, and start the VLLM server using the model `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B`.\n\n**Installation and Cleanup:**\n- The necessary packages (`vllm`, `transformers`, `accelerate`, and `datasets`) are installed in a cell with hidden output to keep the notebook clean.\n- Any previously running VLLM server instances are terminated before starting a new one. This prevents multiple servers from running simultaneously.\n\n**Server Initialization:**\n- The server is launched as a background process using `subprocess.Popen`.\n- **Initialization Time:**  \n  The server typically takes about **1 minute** to fully initialize.\n- **GPU Memory Utilization:**  \n  Monitor your GPU memory usage. Initially, it will be at **0 GB**, and then it will gradually increase until it reaches approximately **12 GB** when the server is fully up and running.\n\nPlease wait until the GPU memory stabilizes around **12 GB** before proceeding to the next steps.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import subprocess\nimport os\n\n# Kill any running VLLM server instances for the specified model\nkill_cmd = \"pkill -f 'vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\"\nsubprocess.run(kill_cmd, shell=True)\n\n# Command to start the VLLM server\ncmd = [\n    \"vllm\", \"serve\", \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n    \"--port\", \"8000\", \"--dtype=half\", \"--max-model-len\", \"5192\"\n]\n\n# Redirect all output to os.devnull to suppress logs\nwith open(os.devnull, \"w\") as devnull:\n    server_process = subprocess.Popen(cmd, stdout=devnull, stderr=devnull)\n\nprint(\"Server started!\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T08:49:38.239858Z",
     "iopub.execute_input": "2025-05-23T08:49:38.240152Z",
     "iopub.status.idle": "2025-05-23T08:49:39.637659Z",
     "shell.execute_reply.started": "2025-05-23T08:49:38.240125Z",
     "shell.execute_reply": "2025-05-23T08:49:39.636682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Server started!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "* you can debug last cell if doesn't work right with this cell (if that works you DO NOT run this cell)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# !vllm serve \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"   --port 8000   --dtype=half   --max-model-len 5192",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T08:49:39.640413Z",
     "iopub.execute_input": "2025-05-23T08:49:39.640784Z",
     "iopub.status.idle": "2025-05-23T08:49:39.645564Z",
     "shell.execute_reply.started": "2025-05-23T08:49:39.640756Z",
     "shell.execute_reply": "2025-05-23T08:49:39.644372Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "# Helper Functions Overview\n\nThis section contains a series of helper functions designed to facilitate the evaluation of mathematical problem solving using the MATH-500 dataset and a local LLM server. These functions handle tasks such as dataset loading, answer extraction, normalization of various mathematical expressions, answer comparison, and result management. Below is an explanation of each group of functions:\n\n---\n\n## Dataset Loading\n\n- **`load_math500_dataset()`**  \n  Loads the test split of the MATH-500 dataset from the Hugging Face repository (`HuggingFaceH4/MATH-500`). This dataset provides the math problems and corresponding solutions used for evaluation.\n\n---\n\n## Answer Extraction\n\n- **`extract_answer(response: str) -> Optional[str]`**  \n  Searches the provided text for the last occurrence of the LaTeX command `\\boxed{...}` and extracts the content within it. This function is essential for retrieving the final answer from the formatted solutions.\n\n---\n\n## Normalization Functions\n\nNormalization functions standardize the format of mathematical expressions to enable accurate comparisons between predicted and correct answers. These functions account for various representations, ensuring that equivalent answers written in different formats are recognized as equal.\n\n- **`normalize_number(num_str: str) -> str`**  \n  Cleans and normalizes numeric strings by removing extraneous characters (e.g., commas, currency symbols, and measurement units) and formatting them into a consistent number format.\n\n- **`numerically_equal(str1: str, str2: str) -> bool`**  \n  Checks if two numeric strings represent the same value within a small tolerance, accounting for floating point precision issues.\n\n- **`normalize_fraction(fraction_str: str) -> str`**  \n  Converts various representations of fractions (with or without braces or using a slash) into a standard LaTeX format: `\\frac{numerator}{denominator}`.\n\n- **`normalize_matrix_entry(entry: str) -> str`**  \n  Standardizes individual matrix entries, especially handling fractions and slash-separated numbers, to ensure consistency within matrix representations.\n\n- **`normalize_matrix(matrix_str: str) -> str`**  \n  Processes a LaTeX matrix (formatted with `\\begin{pmatrix}` and `\\end{pmatrix}`) by normalizing each row and each entry using the matrix entry normalization.\n\n- **`normalize_algebraic_expression(expr: str) -> str`**  \n  Standardizes algebraic expressions by handling coefficients, variables, exponents, and special terms like π (pi). This helps compare algebraic answers regardless of minor formatting differences.\n\n- **`normalize_interval_bound(bound: str) -> str`**  \n  Normalizes the boundary of an interval, ensuring that symbols like infinity (`\\infty`) and other numeric boundaries are consistently formatted.\n\n- **`normalize_interval(interval_str: str) -> str`**  \n  Standardizes an interval provided in LaTeX, ensuring that both bounds are normalized and that the overall format (including brackets) is consistent.\n\n- **`normalize_ordered_tuple(tuple_str: str) -> str`**  \n  Normalizes an ordered tuple by splitting its elements and applying answer normalization to each component, ensuring a standard tuple representation.\n\n- **`normalize_answer(answer: str) -> str`**  \n  The central normalization function that applies the various normalization steps to a given answer. It cleans up LaTeX formatting, removes unnecessary spaces, and calls the specialized normalization functions to standardize numeric, fractional, algebraic, and other mathematical expressions.\n\n---\n\n## Answer Comparison\n\n- **`compare_answers(correct_answer: str, predicted_answer: Optional[str]) -> bool`**  \n  Compares the normalized versions of the correct answer and the predicted answer. This function ensures that answers are compared in a standardized format so that minor differences in formatting do not affect the evaluation outcome.\n\n---\n\n## Result Management Functions\n\nThese functions handle saving and analyzing the results of the evaluation process.\n\n- **`load_existing_results(filename: str) -> list[Dict]`**  \n  Loads previously saved evaluation results from a JSON file. If the file does not exist, it returns an empty list.\n\n- **`save_result(filename: str, result: Dict)`**  \n  Appends a single evaluation result (including problem details, the LLM response, and correctness) to the results file in JSON format.\n\n- **`analyze_results(results: list[Dict])`**  \n  Analyzes the evaluation outcomes by summarizing the total number of problems, counting the correct answers, calculating the accuracy, and printing details for any problems that were answered incorrectly.\n\n---\n\n## Main Evaluation and Response Handling\n\n- **`evaluate()`**  \n  The primary function that orchestrates the evaluation process:\n  - Creates a results directory if it doesn't already exist.\n  - Loads the MATH-500 dataset.\n  - Iterates over each problem (while skipping already processed ones).\n  - Sends the problem text to the local LLM server using `get_llm_response`.\n  - Extracts and compares the answers, then saves the result.\n  - Finally, it analyzes and prints a summary of the evaluation.\n\n- **`get_llm_response(prompt: str) -> str`**  \n  Sends a prompt to the locally running LLM server (via an HTTP POST request to `http://localhost:8000/v1/chat/completions`) and returns the server's response. This function is key to obtaining the model's predicted answer.\n",
   "metadata": {
    "id": "6GpdRCdxgTLK"
   }
  },
  {
   "cell_type": "code",
   "source": "import json\nimport os\nimport re\nfrom typing import Dict, Optional, Union\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport torch\n\n# Load the MATH-500 dataset\ndef load_math500_dataset():\n    dataset = load_dataset(\"HuggingFaceH4/MATH-500\")[\"test\"]\n    return dataset\n\n# Extract the last boxed answer from text\ndef extract_answer(response: str) -> Optional[str]:\n    if not response:\n        return None\n    start_idx = response.rfind('\\\\boxed{')\n    if start_idx == -1:\n        return None\n    brace_count = 1\n    pos = start_idx + 7  # length of '\\boxed{'\n    while pos < len(response) and brace_count > 0:\n        if response[pos] == '{':\n            brace_count += 1\n        elif response[pos] == '}':\n            brace_count -= 1\n        pos += 1\n    if brace_count == 0:\n        answer = response[start_idx + 7:pos - 1]\n        return answer.strip()\n    return None\n\n# Normalization and comparison functions (unchanged from original)\ndef normalize_number(num_str: str) -> str:\n    try:\n        cleaned = re.sub(r'[,\\$\\\\]|\\s*(?:cm|m|kg|ft|in|lb|oz|ml|L)$|\\s*\\\\text{[^}]+}', '', num_str).strip()\n        if cleaned.startswith('.'):\n            cleaned = '0' + cleaned\n        num = float(cleaned)\n        if abs(num) < 1 and '.' in cleaned:\n            decimal_places = len(cleaned.split('.')[1])\n            format_str = f\"{{:.{decimal_places}f}}\"\n            result = format_str.format(num)\n        else:\n            result = str(num)\n        return result\n    except:\n        return num_str\n\ndef numerically_equal(str1: str, str2: str) -> bool:\n    try:\n        return abs(float(str1) - float(str2)) < 1e-10\n    except:\n        return False\n\ndef normalize_fraction(fraction_str: str) -> str:\n    try:\n        fraction_str = fraction_str.replace('\\\\dfrac', '\\\\frac')\n        fraction_str = ''.join(fraction_str.split())\n        fraction_str = re.sub(r'\\s*\\\\text{[^}]+}', '', fraction_str)\n        mixed_brace = re.match(r'^\\\\frac(\\d+)\\{(\\d+)\\}$', fraction_str)\n        if mixed_brace:\n            num, den = mixed_brace.groups()\n            return f\"\\\\frac{{{num}}}{{{den}}}\"\n        no_braces = re.match(r'^\\\\frac(\\d+)(\\d+)$', fraction_str)\n        if no_braces:\n            num, den = no_braces.groups()\n            return f\"\\\\frac{{{num}}}{{{den}}}\"\n        if '/' in fraction_str and not any(c in fraction_str for c in '\\\\{}'):\n            num, den = fraction_str.split('/')\n            return f\"\\\\frac{{{num.strip()}}}{{{den.strip()}}}\"\n        standard = re.match(r'^\\\\frac\\{([^{}]+)\\}\\{([^{}]+)\\}$', fraction_str)\n        if standard:\n            num, den = standard.groups()\n            return f\"\\\\frac{{{num}}}{{{den}}}\"\n    except:\n        return fraction_str\n\ndef normalize_matrix_entry(entry: str) -> str:\n    entry = ''.join(entry.split())\n    if '/' in entry and not any(c in entry for c in '\\\\{}'):\n        if entry.startswith('-'):\n            num, den = entry[1:].split('/')\n            return f\"-{num.strip()}/{den.strip()}\"\n        else:\n            num, den = entry.split('/')\n            return f\"{num.strip()}/{den.strip()}\"\n    entry = entry.replace('\\\\dfrac', '\\\\frac')\n    frac_match = re.match(r'^(-)?\\\\frac\\{(\\d+)\\}\\{(\\d+)\\}$', entry)\n    if frac_match:\n        sign, num, den = frac_match.groups()\n        sign = sign if sign else ''\n        return f\"{sign}{num}/{den}\"\n    return entry\n\ndef normalize_matrix(matrix_str: str) -> str:\n    try:\n        matrix_str = ''.join(matrix_str.split())\n        match = re.match(r'^\\\\begin\\{pmatrix\\}(.*?)\\\\end\\{pmatrix\\}$', matrix_str)\n        if not match:\n            return matrix_str\n        content = match.group(1)\n        rows = content.split('\\\\\\\\')\n        normalized_rows = []\n        for row in rows:\n            if '&' in row:\n                entries = [normalize_matrix_entry(entry) for entry in row.split('&')]\n            else:\n                entries = [normalize_matrix_entry(row)]\n            normalized_rows.append('&'.join(entries))\n        result = \"\\\\begin{pmatrix}\" + \"\\\\\\\\\".join(normalized_rows) + \"\\\\end{pmatrix}\"\n        return result\n    except:\n        return matrix_str\n\ndef normalize_algebraic_expression(expr: str) -> str:\n    try:\n        expr = ''.join(expr.split())\n        monomial_match = re.match(r'^(-?\\d*\\.?\\d*)?([a-zA-Z])(?:\\^(-?\\d+))?$', expr)\n        if monomial_match:\n            coeff, var, exp = monomial_match.groups()\n            coeff = coeff if coeff and coeff not in ['+', '-'] else ('1' if not coeff else '-1')\n            exp = exp if exp else '1'\n            if coeff == '1' and exp == '1':\n                return var\n            elif coeff == '1':\n                return f\"{var}^{exp}\"\n            elif coeff == '-1' and exp == '1':\n                return f\"-{var}\"\n            elif coeff == '-1':\n                return f\"-{var}^{exp}\"\n            elif exp == '1':\n                return f\"{coeff}{var}\"\n            else:\n                return f\"{coeff}{var}^{exp}\"\n        pi_term_match = re.match(r'^(-?\\d*\\.?\\d*)\\\\?pi$', expr)\n        if pi_term_match:\n            coeff = pi_term_match.group(1)\n            if not coeff or coeff == '-':\n                coeff = '-1' if coeff == '-' else '1'\n            return f\"{coeff}\\\\pi\"\n        frac_pi_match = re.match(r'^\\\\frac{([^{}]+)}{([^{}]+)}\\\\?pi$', expr)\n        if frac_pi_match:\n            num, den = frac_pi_match.groups()\n            return f\"\\\\frac{{{num}}}{{{den}}}\\\\pi\"\n        frac_match = re.match(r'^\\\\frac{([^{}]+)}{([^{}]+)}$', expr)\n        if frac_match:\n            num, den = frac_match.groups()\n            return f\"\\\\frac{{{num}}}{{{den}}}\"\n    except:\n        return expr.lower()\n\ndef normalize_interval_bound(bound: str) -> str:\n    if '\\\\infty' in bound:\n        sign = '-' if bound.startswith('-') else ''\n        return f\"{sign}\\\\infty\"\n    return normalize_answer(bound) or bound\n\ndef normalize_interval(interval_str: str) -> str:\n    try:\n        interval_str = ''.join(interval_str.split())\n        match = re.match(r'^\\\\left?([\\[\\(])(.*?),(.*?)\\\\right?([\\]\\)])$', interval_str)\n        if not match:\n            match = re.match(r'^([\\[\\(])(.*?),(.*?)([\\]\\)])$', interval_str)\n            if not match:\n                return interval_str\n        left_bracket, left_bound, right_bound, right_bracket = match.groups()\n        norm_left = normalize_interval_bound(left_bound)\n        norm_right = normalize_interval_bound(right_bound)\n        return f\"\\\\left{left_bracket}{norm_left},{norm_right}\\\\right{right_bracket}\"\n    except:\n        return interval_str\n\ndef normalize_ordered_tuple(tuple_str: str) -> str:\n    try:\n        tuple_str = tuple_str.replace('\\\\dfrac', '\\\\frac')\n        tuple_str = tuple_str.replace('\\\\left', '').replace('\\\\right', '')\n        tuple_str = re.sub(r'\\\\?\\s+', '', tuple_str)\n        inner = tuple_str.strip('()')\n        parts = inner.split(',')\n        normalized_parts = [normalize_answer(part.strip()) for part in parts if normalize_answer(part.strip())]\n        return f\"({','.join(normalized_parts)})\"\n    except:\n        return None\n\ndef normalize_answer(answer: str) -> str:\n    if answer is None:\n        return \"\"\n    answer = re.sub(r'\\\\text{[^}]+(?:inches|feet|meters|cm|m|kg|ft|in|lb|oz|ml|L|per|second|minute|hour)[^}]*}', '', answer)\n    answer = re.sub(r'(?<!\\\\)\\s+', '', answer)\n    ordered_pair_match = re.match(r'^(?:\\\\left)?\\((.*?)(?:\\\\right)?\\)$', answer)\n    if ordered_pair_match:\n        content = ordered_pair_match.group(1)\n        parts = content.split(',')\n        normalized_parts = [normalize_answer(part) for part in parts if normalize_answer(part)]\n        return f\"({','.join(normalized_parts)})\"\n    answer = ''.join(answer.split())\n    if not answer:\n        return None\n    pm_match = re.match(r'^(.*?)(?:\\\\pm|-)(.*?)$', answer)\n    if pm_match:\n        left, right = pm_match.groups()\n        norm_left = normalize_answer(left) if left else \"\"\n        norm_right = normalize_answer(right) if right else \"\"\n        if norm_left or norm_right:\n            return f\"{norm_left}\\\\pm{norm_right}\"\n    trig_match = re.match(r'^\\\\(?:sin|cos|tan|cot|sec|csc)\\s*([a-zA-Z])$', answer)\n    if trig_match:\n        variable = trig_match.group(1)\n        func_name = re.match(r'^\\\\(.*?)(?:\\s|$)', answer).group(1)\n        return f\"\\\\{func_name}{variable}\"\n    text_match = re.match(r'^(?:\\\\text{)?([A-Za-z]+)(?:})?$', answer)\n    if text_match:\n        return text_match.group(1).lower()\n    if (answer.startswith('\\\\left[') or answer.startswith('\\\\left(') or\n        answer.startswith('[') or answer.startswith('(')) and \\\n       (answer.endswith('\\\\right]') or answer.endswith('\\\\right)') or\n        answer.endswith(']') or answer.endswith(')')):\n        return normalize_interval(answer)\n    if answer.startswith('\\\\begin{pmatrix}') and answer.endswith('\\\\end{pmatrix}'):\n        return normalize_matrix(answer)\n    answer = answer.replace('\\\\dfrac', '\\\\frac')\n    if '\\\\frac' in answer or '/' in answer:\n        return normalize_fraction(answer)\n    neg_sqrt_match = re.match(r'^-\\\\sqrt\\{?(\\d+)\\}?$', answer)\n    if neg_sqrt_match:\n        num = neg_sqrt_match.group(1)\n        return f\"-\\\\sqrt{{{num}}}\"\n    sqrt_match = re.match(r'^(\\d*)?\\\\sqrt\\{?(\\d+)\\}?$', answer)\n    if sqrt_match:\n        coeff, num = sqrt_match.groups()\n        coeff = coeff if coeff else '1'\n        return f\"\\\\sqrt{{{num}}}\" if coeff == '1' else f\"{coeff}\\\\sqrt{{{num}}}\"\n    sqrt_with_coeff_match = re.match(r'^(\\d+)\\\\sqrt\\{?(\\d+)\\}?$', answer)\n    if sqrt_with_coeff_match:\n        coeff, num = sqrt_with_coeff_match.groups()\n        return f\"{coeff}\\\\sqrt{{{num}}}\"\n    base_match = re.match(r'^(\\d+)(?:_\\{?(\\d+)\\}?|_(\\d+))$', answer)\n    if base_match:\n        number, base1, base2 = base_match.groups()\n        base = base1 if base1 else base2\n        return f\"{number}_{base}\"\n    percent_match = re.match(r'^(\\d+(?:\\.\\d*)?)\\s*\\\\?%$', answer)\n    if percent_match:\n        return normalize_number(percent_match.group(1))\n    unit_match = re.match(r'^(\\d+(?:\\.\\d*)?)\\s*(?:(?:\\\\[,\\s])|,)?\\s*(?:\\\\\\\\)?(?:\\\\text{(\\w+)}|\\\\?(?:cm|m|kg|ft|in|lb|oz|ml|L))$', answer)\n    if unit_match:\n        return normalize_number(unit_match.group(1))\n    currency_match = re.match(r'^\\\\?\\$?([\\d,]+\\.?\\d*)$', answer)\n    if currency_match:\n        return normalize_number(currency_match.group(1))\n    if re.match(r'^-?[\\d,]+$', answer):\n        return normalize_number(answer)\n    unit_match = re.match(r'^(-?[\\d,]+(?:\\.\\d*)?)\\s*(?:\\\\(?:mbox|text|hbox|displaystyle)\\{[^}]+\\})?(?:\\^?\\d)?$', answer)\n    if unit_match:\n        return normalize_number(unit_match.group(1))\n    mc_match = re.match(r'^\\\\text{\\(?([A-Za-z])\\)?}$|^\\(?([A-Za-z])\\)?$', answer)\n    if mc_match:\n        return (mc_match.group(1) or mc_match.group(2)).lower()\n    degree_match = re.match(r'^(-?[\\d,]+(?:\\.\\d*)?)\\s*(?:(?:\\^?\\\\circ)|(?:{\\\\circ})|(?:°))?$', answer)\n    if degree_match:\n        return normalize_number(degree_match.group(1))\n    answer = re.sub(r'\\\\text{([^{}]+)}', r'\\1', answer)\n    try:\n        return normalize_algebraic_expression(answer)\n    except:\n        pass\n    answer = answer.replace('\\\\left', '').replace('\\\\right', '')\n    answer = answer.replace('\\\\(', '(').replace('\\\\)', ')')\n    answer = answer.replace('\\\\[', '[').replace('\\\\]', ']')\n    answer = answer.replace('\\\\{', '{').replace('\\\\}', '}')\n    answer = re.sub(r'\\\\sqrt\\{?(\\d+)\\}?', r'\\\\sqrt{\\1}', answer)\n    answer = re.sub(r'\\\\sqrt{([^{}]+)}', r'\\\\sqrt\\1', answer)\n    if re.match(r'^\\d+\\\\%$', answer) or re.match(r'^\\d+$', answer):\n        answer = re.sub(r'\\\\%$', '', answer)\n    answer = re.sub(r'\\\\text{([^{}]+)}', r'\\1', answer)\n    while len(answer) >= 2 and answer[0] == '{' and answer[-1] == '}':\n        if '\\\\frac' in answer:\n            break\n        answer = answer[1:-1]\n    return answer.lower() if answer else None\n\ndef compare_answers(correct_answer: str, predicted_answer: Optional[str]) -> bool:\n    if predicted_answer is None:\n        return False\n    if numerically_equal(correct_answer, predicted_answer):\n        return True\n    normalized_correct = normalize_answer(correct_answer)\n    normalized_predicted = normalize_answer(predicted_answer)\n    if not normalized_correct or not normalized_predicted:\n        return False\n    if normalized_correct == \"\" and normalized_predicted == \"\":\n        return False\n    if ('\\\\left[' in normalized_correct or '\\\\left(' in normalized_correct) and \\\n       ('\\\\left[' in normalized_predicted or '\\\\left(' in normalized_predicted):\n        return normalized_correct == normalized_predicted\n    return normalized_correct == normalized_predicted\n\n# Load existing results\ndef load_existing_results(filename: str) -> list[Dict]:\n    try:\n        with open(filename, 'r') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        return []\n\n# Save a single result\ndef save_result(filename: str, result: Dict):\n    results = load_existing_results(filename)\n    results.append(result)\n    with open(filename, 'w') as f:\n        json.dump(results, f, indent=2)\n\n# Analyze and print results\ndef analyze_results(results: list[Dict]):\n    total = len(results)\n    correct = sum(1 for r in results if r['is_correct'])\n    accuracy = correct / total if total > 0 else 0\n    print(\"\\n=== Results Summary ===\")\n    print(f\"Total problems: {total}\")\n    print(f\"Correct answers: {correct}\")\n    print(f\"Accuracy: {accuracy:.2%}\")\n    print(\"\\n=== Incorrect Problems ===\")\n    for r in results:\n        if not r['is_correct']:\n            print(f\"Problem {r['index']}:\")\n            print(f\"Expected: {r['correct_answer']}\")\n            print(f\"Predicted: {r['predicted_answer']}\")\n            print(\"---\")\n\n# Main evaluation function\ndef evaluate():\n    os.makedirs(\"results\", exist_ok=True)\n    results_file = \"evaluation_results_math500_deepseek.json\"\n    dataset = load_math500_dataset()\n    existing_results = load_existing_results(results_file)\n    processed_indexes = {result['index'] for result in existing_results}\n    cnt = 0\n    t=0\n    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n        if idx in processed_indexes:\n            continue\n        t += 1\n        problem_text = item['problem']\n        correct_answer = extract_answer(item['solution'])  # Extract from 'solution', not 'answer'\n        response = get_llm_response(problem_text)\n        predicted_answer = extract_answer(response)\n        is_correct = compare_answers(correct_answer, predicted_answer)\n        result = {\n            \"index\": idx,\n            \"problem\": problem_text,\n            \"response\": response,\n            \"correct_answer\": correct_answer,\n            \"predicted_answer\": predicted_answer,\n            \"is_correct\": is_correct\n        }\n        save_result(results_file, result)\n        if is_correct:\n          cnt += 1\n        print(f\"cnt :  {cnt} idx: {t}\")\n    final_results = load_existing_results(results_file)\n    analyze_results(final_results)\n\n",
   "metadata": {
    "id": "jgnyeyBxE2Ci",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T08:49:39.646429Z",
     "iopub.execute_input": "2025-05-23T08:49:39.646712Z",
     "iopub.status.idle": "2025-05-23T08:49:44.858604Z",
     "shell.execute_reply.started": "2025-05-23T08:49:39.646668Z",
     "shell.execute_reply": "2025-05-23T08:49:44.857558Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "## LLM Query Function\n\n* This Python function sends prompts to a locally-hosted LLM API and returns the generated response\n* you can change max_tokens and temperature as you want",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import requests\ndef get_llm_response(prompt, max_tokens=1700, temperature=0.3):\n    \"\"\" get response from Qwen model \"\"\"\n    url = \"http://localhost:8000/v1/chat/completions\"\n\n    payload = {\n        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n\n        ],\n    \"max_tokens\": max_tokens,\n    \"temperature\": temperature\n    }\n    response = requests.post(url, json=payload)\n    return response.json()['choices'][0]['message']['content'].strip()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T08:49:44.859571Z",
     "iopub.execute_input": "2025-05-23T08:49:44.859969Z",
     "iopub.status.idle": "2025-05-23T08:49:44.865517Z",
     "shell.execute_reply.started": "2025-05-23T08:49:44.859946Z",
     "shell.execute_reply": "2025-05-23T08:49:44.864593Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": "## Test Prompt: Evaluating an Integral\n\nIn this cell, we define a new math benchmark question to verify that the LLM server is correctly set up and that responses can be retrieved.\n\n**Question:**  \nWhat is the value of the integral  \n$$\\int_0^1 x^2\\,dx$$  \n\n**Expected Answer:**  \n$$\\boxed{\\frac{1}{3}}$$\n\nThe cell sends this prompt to the LLM server using the `get_llm_response` function and prints the response. This helps confirm that the integration between the notebook and the LLM server is working properly.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "os.environ[\"LOG_LEVEL\"] = \"WARNING\"\n# Define a new math benchmark question for testing\nquestion = \"What is the value of the integral $$\\\\int_0^1 x^2 dx$$ answer it directly in one sentence?\"\n# Real answer: \\boxed{\\frac{1}{3}}\n\n# Get response from the LLM server using the provided get_llm_response function\nresponse = get_llm_response(question)\n\n# Print the response to verify that the setup is working correctly\nprint(\"Response:\", response)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T08:57:11.060411Z",
     "iopub.execute_input": "2025-05-23T08:57:11.060988Z",
     "iopub.status.idle": "2025-05-23T08:57:15.057609Z",
     "shell.execute_reply.started": "2025-05-23T08:57:11.060962Z",
     "shell.execute_reply": "2025-05-23T08:57:15.056894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Response: To evaluate the integral of \\( x^2 \\) from 0 to 1, I start by finding the antiderivative of \\( x^2 \\), which is \\( \\frac{x^3}{3} \\).\n\nNext, I apply the Fundamental Theorem of Calculus by substituting the upper limit 1 into the antiderivative and subtracting the value of the antiderivative at the lower limit 0.\n\nThis results in \\( \\frac{1^3}{3} - \\frac{0^3}{3} = \\frac{1}{3} \\).\n\nTherefore, the value of the integral is \\( \\frac{1}{3} \\).\n</think>\n\nThe value of the integral is \\(\\boxed{\\dfrac{1}{3}}\\).\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": "# Customizable CoT Prompt Template\n* modify cot prompt then evaluate on math benchmark\n",
   "metadata": {
    "id": "dNUWF2U_HC8g"
   }
  },
  {
   "cell_type": "code",
   "source": "import requests\n\n# Define the system prompt\nCOT_PROMPT = '''You are solving mathematics problems.\n\nPlease think step by step.\n\nImportant: Always end your solution with the final answer in this format:\n\n\\\\[\n\\\\boxed{your_answer_here}\n\\\\]\n\nThe entire answer should be contained completely within the \\\\boxed{} command.'''\n\ndef get_COT_response(problem):\n    prompt = COT_PROMPT + \"\\n\" + problem\n    url = \"http://localhost:8000/v1/chat/completions\"\n\n    payload = {\n        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n\n        ],\n    \"max_tokens\": 1900,\n    \"temperature\": 0.3\n    }\n    response = requests.post(url, json=payload)\n    return response.json()['choices'][0]['message']['content'].strip()",
   "metadata": {
    "id": "58w5yeo9GrP2",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-22T16:29:50.751608Z",
     "iopub.execute_input": "2025-05-22T16:29:50.752037Z",
     "iopub.status.idle": "2025-05-22T16:29:50.757023Z",
     "shell.execute_reply.started": "2025-05-22T16:29:50.752015Z",
     "shell.execute_reply": "2025-05-22T16:29:50.756351Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": "# Evaluate CoT\n* modify response generation part to evalute this method.",
   "metadata": {
    "id": "Una6ucDAHb2_"
   }
  },
  {
   "cell_type": "code",
   "source": "def evaluate_cot():\n    os.makedirs(\"results\", exist_ok=True)\n    results_file = \"evaluation_results_math500_deepseek_cot.json\"\n    dataset = load_math500_dataset()\n    existing_results = load_existing_results(results_file)\n    processed_indexes = {result['index'] for result in existing_results}\n    cnt = 0\n    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n        if idx in processed_indexes:\n            continue\n        if idx >= 30:\n          break\n        problem_text = item['problem']\n        correct_answer = extract_answer(item['solution'])\n        ##########################################################\n        response = get_COT_response(problem_text)\n        predicted_answer = extract_answer(response)\n        ##########################################################\n        is_correct = compare_answers(correct_answer, predicted_answer)\n        result = {\n            \"index\": idx,\n            \"problem\": problem_text,\n            \"response\": response,\n            \"correct_answer\": correct_answer,\n            \"predicted_answer\": predicted_answer,\n            \"is_correct\": is_correct\n        }\n        save_result(results_file, result)\n        if is_correct:\n          cnt += 1\n        print(f\"corrects :  {cnt} idx: {idx}\")\n    final_results = load_existing_results(results_file)\n    analyze_results(final_results)",
   "metadata": {
    "id": "Na6kksqZHbOq",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-22T16:30:43.659213Z",
     "iopub.execute_input": "2025-05-22T16:30:43.659475Z",
     "iopub.status.idle": "2025-05-22T16:30:43.665883Z",
     "shell.execute_reply.started": "2025-05-22T16:30:43.659458Z",
     "shell.execute_reply": "2025-05-22T16:30:43.665018Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "evaluate_cot()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJbLqPXYOLvm",
    "outputId": "c8d30658-7594-4745-e4c2-3595c9a0acfb",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-22T16:30:48.345809Z",
     "iopub.execute_input": "2025-05-22T16:30:48.346477Z",
     "iopub.status.idle": "2025-05-22T16:40:56.267973Z",
     "shell.execute_reply.started": "2025-05-22T16:30:48.346455Z",
     "shell.execute_reply": "2025-05-22T16:40:56.267206Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "=== Results Summary ===\n",
    "\n",
    "Total problems: 30\n",
    "\n",
    "Correct answers: 15\n",
    "\n",
    "Accuracy: 50.00%\n",
    "\n",
    "Runtime: 10:06"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Best-of-N \n\nThe **Best-of-N** approach improves math problem-solving by generating *N* solutions and selecting the one with the highest average token log-likelihood. Each solution is crafted using a prompt that encourages step-by-step reasoning and includes a formatted answer. The final selected response is both reliable and well-presented.\n\n### Steps\n1. **Generate**: Produce *N* responses using a structured guiding prompt.\n2. **Evaluate**: Compute the average log-likelihood for each response based on token probabilities.\n3. **Select**: Identify and choose the response with the highest score.\n\nThis method ensures a statistically robust and clearly formatted solution.\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Verification Methods in Best‑of‑N Evaluation\n\nWhen sampling multiple candidate solutions for each math problem, we need a reliable way to choose the single best answer. We support two complementary approaches:\n\n### Log‑Probability Scoring\n\n**Concept**  \nEach generated solution comes with token‑level log‑likelihoods. By averaging these values across all tokens in the response, we obtain a single score reflecting how “confident” the model is in that entire output.\n\n**Why Use It**  \n- **Self‑Contained & Fast**: Requires no external calls or additional models.  \n- **Cost‑Effective**: Purely internal computation, so it adds negligible expense.  \n\n**Limitations**  \n- A high likelihood does not always imply a correct or well‑reasoned solution, especially on complex math problems.\n\n---\n\n### LLM‑Based Verification\n\n**Concept**  \nInstead of trusting raw likelihoods, we hand all sampled responses off to a second, high‑quality language model (e.g. Gemini Mini). That model reads the original problem and the list of candidate boxed answers, then selects the one it judges to be correct.\n\n**Why Use It**  \n- **Deeper Reasoning**: A dedicated verifier can compare alternative answers and catch subtle mistakes.  \n- **Improved Robustness**: Mitigates cases where a flawed but high‑probability output would otherwise be chosen.\n\n**Trade‑Offs**  \n- **Slower**: Requires additional API calls and round‑trip latency.  \n- **External Cost**: Incurs usage fees on the verification model.\n\n---\n\n### Balancing Speed, Cost, and Accuracy\n\nBy exposing a simple toggle between these two methods, you can:\n\n- **Optimize for Speed**: Use log‑prob scoring when you need rapid, low‑cost evaluation.  \n- **Optimize for Accuracy**: Use LLM‑based verification when correctness is paramount.  \n\nExperiment on your dataset to find the right trade‑off for your needs.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=\"YOUR_API_KEY_HERE\",  # Replace with your actual API key\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/\"\n",
    ")\n",
    "\n",
    "# Use the cheapest Gemini model\n",
    "LLM_API_MODEL = \"gemini-2.0-flash-lite\"\n",
    "\n",
    "def get_api_response(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Send `prompt` to Gemini and return its reply.\n",
    "    \"\"\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=LLM_API_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "# test the model\n",
    "print(get_api_response(\"hi. how you doing?\"))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T08:57:38.353826Z",
     "iopub.execute_input": "2025-05-23T08:57:38.354574Z",
     "iopub.status.idle": "2025-05-23T08:57:40.035020Z",
     "shell.execute_reply.started": "2025-05-23T08:57:38.354544Z",
     "shell.execute_reply": "2025-05-23T08:57:40.034410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "I'm doing well, thank you for asking! As a large language model, I don't experience emotions, but I am functioning optimally and ready to assist you. How can I help you today?\n\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "import re\nimport time\nimport math\nimport heapq\nimport requests\nfrom typing import List, Optional\n\n\nSYSTEM_PROMPT = '''You are solving mathematics problems.\n\nPlease think step by step.\n\nImportant: Always end your solution with the final answer in this format:\n\n\\\\[\n\\\\boxed{your_answer_here}\n\\\\]\n\nThe entire answer should be contained completely within the \\\\boxed{} command.'''\n\n\ndef verify_with_gemini(problem: str, outputs: List[str]) -> Optional[str]:\n    \"\"\"\n    Given the original problem and a list of candidate full-response texts,\n    asks a Gemini API to pick the correct final answer (boxed).\n    \"\"\"\n    # Deduplicate `outputs` into a `unique_answers: List[str]` by extracting\n    unique_answers = []\n    for output in outputs:\n        answer = extract_answer(output)\n        if answer is None:\n            answer = \"<no_boxed_answer>\"\n        if answer not in unique_answers:\n            unique_answers.append(answer)\n\n    # Build `options` as numbered lines of the form \"1. \\\\boxed{...}\" from `unique_answers`.\n    options = \"\\n\".join([f\"{i+1}. \\\\boxed{{{ans}}}\" for i, ans in enumerate(unique_answers)])\n    # Compose `verify_prompt` with the problem and options.\n    verify_prompt = f\"\"\"Given the problem: {problem}\\n\\n\nHere are the candidate answers:\\n{options}\\n\\n Choose the correct answer. If none are correct, choose 0.\"\"\" + '''Important: Always end your solution with the final answer in this format:\n\n\\\\[\n\\\\boxed{your_answer_here}\n\\\\]\n\nThe entire answer should be contained completely within the \\\\boxed{} command.'''\n\n    # Call `get_api_response(verify_prompt)` and strip whitespace → `chosen`.\n    chosen = get_api_response(verify_prompt).strip()\n\n    # Return `extract_answer(chosen)` to normalize formatting.\n    return extract_answer(chosen)\n\n\ndef best_of_n_response(\n    problem: str,\n    N: int = 5,\n    use_logprob: bool = True,\n    model: str = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n    port: int = 8000,\n    temp: float = 0.2\n) -> Optional[str]:\n    \"\"\"\n    Run N samples on your VLLM server, then:\n    - if use_logprob: pick the candidate with highest avg log-prob\n    - else: hand off all N outputs to Gemini to choose the best boxed answer\n    \"\"\"\n    url = f\"http://localhost:{port}/v1/chat/completions\"\n    prompt = SYSTEM_PROMPT + \"\\n\" + problem\n\n    samples = []\n    for _ in range(N):\n        # Build the `payload` dict with model, messages, max_tokens, temperature, and logprobs.\n        payload = {\n            \"model\": model,\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }\n            ],\n            \"max_tokens\": 1900,\n            \"temperature\": temp,\n            \"logprobs\": True,\n            \"top_logprobs\": 1\n        }\n        # POST to `requests.post(url, json=payload)` and parse `.json()` → `resp`.\n        response = requests.post(url, json=payload)\n        resp = response.json()\n        # Extract `text` from `resp['choices'][0]['message']['content']`.\n        text = resp['choices'][0]['message']['content']\n\n        # Compute `avg_lp` by collecting all `choice['logprobs']['content'][*]['logprob']` values.\n        logprobs = resp['choices'][0]['logprobs']['content']\n        avg_lp = sum([item['logprob'] for item in logprobs]) / len(logprobs) if logprobs else float('-inf')\n\n        # Append `{\"text\": text, \"avg_lp\": avg_lp}` to `samples`.\n        samples.append({\"text\": text, \"avg_lp\": avg_lp})\n\n    if use_logprob:\n        # Select the `sample` with the highest `avg_lp`.\n        best = max(samples, key=lambda x: x[\"avg_lp\"])\n        # Return `extract_answer(best[\"text\"])`.\n        return extract_answer(best[\"text\"])\n    else:\n        # Gather `outs = [s[\"text\"] for s in samples]`.\n        outs = [s[\"text\"] for s in samples]\n        # Return `verify_with_gemini(problem, outs)`.\n        return verify_with_gemini(problem, outs)\n",
   "metadata": {
    "id": "1NylbVYkT1kx",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T08:57:43.468002Z",
     "iopub.execute_input": "2025-05-23T08:57:43.468572Z",
     "iopub.status.idle": "2025-05-23T08:57:43.478581Z",
     "shell.execute_reply.started": "2025-05-23T08:57:43.468547Z",
     "shell.execute_reply": "2025-05-23T08:57:43.477929Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": "# Evaluate best of n\n\n* modify response generation part to evalute this method.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def evaluate_best_of_n(use_logprob: bool = True, N: int = 3):\n    os.makedirs(\"results\", exist_ok=True)\n    results_file = (\n        \"evaluation_results_math500_deepseek_best_of_n_logprob.json\"\n        if use_logprob else\n        \"evaluation_results_math500_deepseek_best_of_n_gpt.json\"\n    )\n\n    dataset = load_math500_dataset()\n    existing = load_existing_results(results_file)\n    seen = {r['index'] for r in existing}\n    correct = 0\n\n    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n        if idx in seen or idx >= 30:\n            continue\n\n        prob = item['problem']\n        true_ans = extract_answer(item['solution'])\n        pred_ans = best_of_n_response(prob, N=N, use_logprob=use_logprob)\n        is_corr = compare_answers(true_ans, pred_ans)\n\n        save_result(results_file, {\n            \"index\": idx,\n            \"problem\": prob,\n            \"correct_answer\": true_ans,\n            \"predicted_answer\": pred_ans,\n            \"is_correct\": is_corr\n        })\n        if is_corr:\n            correct += 1\n        print(f\"corrects: {correct} / {idx+1}\")\n\n    analyze_results(load_existing_results(results_file))\n",
   "metadata": {
    "id": "jyCkE9ToOHNO",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T08:57:46.381192Z",
     "iopub.execute_input": "2025-05-23T08:57:46.381762Z",
     "iopub.status.idle": "2025-05-23T08:57:46.387581Z",
     "shell.execute_reply.started": "2025-05-23T08:57:46.381737Z",
     "shell.execute_reply": "2025-05-23T08:57:46.386648Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "# ─── Example calls ─────────────────────────────────────────────────────────\nevaluate_best_of_n(use_logprob=True,  N=3)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "W_6D_Pb8V3tr",
    "outputId": "e87ea8e5-1b4e-40ce-d947-ec6142786be9",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T08:57:49.187792Z",
     "iopub.execute_input": "2025-05-23T08:57:49.188447Z",
     "iopub.status.idle": "2025-05-23T09:31:49.197369Z",
     "shell.execute_reply.started": "2025-05-23T08:57:49.188422Z",
     "shell.execute_reply": "2025-05-23T09:31:49.196702Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "=== Results Summary ===\n",
    "\n",
    "Total problems: 30\n",
    "\n",
    "Correct answers: 15\n",
    "\n",
    "Accuracy: 50.00%\n",
    "\n",
    "Runtime: 33:57"
   ]
  },
  {
   "cell_type": "code",
   "source": "evaluate_best_of_n(use_logprob=False, N=3)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T09:34:09.177652Z",
     "iopub.execute_input": "2025-05-23T09:34:09.178625Z",
     "iopub.status.idle": "2025-05-23T10:10:24.241593Z",
     "shell.execute_reply.started": "2025-05-23T09:34:09.178601Z",
     "shell.execute_reply": "2025-05-23T10:10:24.240811Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "=== Results Summary ===\n",
    "\n",
    "Total problems: 30\n",
    "\n",
    "Correct answers: 19\n",
    "\n",
    "Accuracy: 63.33%\n",
    "\n",
    "Runtime: 36:13"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Beam Search\n\nThis cell implements a beam search strategy for generating candidate reasoning chains. The method generates multiple continuations at each reasoning step, scoring each candidate based on its average token log-likelihood. By retaining and expanding only the top candidates, the approach efficiently searches for the most promising chain-of-thought that leads to the final answer in the required format.\n\n**Key Components:**\n\n- **Model Invocation & Token Scoring:**  \n  The `call_qwen_model_raw` function sends requests to a local Qwen model endpoint using step-specific prompts. It returns generated text together with the average token log-probability, which is used as a quality metric.\n\n- **Candidate Representation:**  \n  The `BeamCandidate` class encapsulates a reasoning chain. It stores the generated text (sequence), cumulative log-probability, per-step scores, token count, and a finished flag (indicating if the candidate contains the final answer).\n\n- **Step-wise Reasoning Generation:**  \n  The `generate_reasoning_steps` function creates multiple candidate continuations for each reasoning step. Different prompts guide the generation for understanding the problem, planning a strategy, and producing the final answer (which is always enclosed in a `\\boxed{}` block).\n\n- **Beam Search Process:**  \n  The `beam_search` function expands candidate chains over several steps. At each step, candidates are updated by appending the new reasoning text and averaging the log-probabilities from all tokens(you can use num_token now). Only the top candidates (based on cumulative score) are retained for further expansion.\n\n- **Final Answer Extraction:**  \n  The `run_qwen_beam_search` function initializes the prompt with the problem statement, runs the beam search, and extracts the final answer from the best candidate if it is complete.\n\nThis structured approach ensures efficient exploration of possible reasoning paths while focusing on the most promising ones to arrive at the final answer in the expected format.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport requests\nfrom typing import Optional, List\nfrom tqdm import tqdm\n\n\ndef score_with_gpt(problem: str, reasoning_step: str) -> float:\n    \"\"\"\n    Ask a high‑quality LLM (via get_api_response) to rate the given\n    reasoning step on a 0–1 scale. Returns the numeric score.\n    \"\"\"\n    prompt = (\n        \"You are a rigorous math reasoning evaluator.\\n\\n\"\n        f\"Problem:\\n{problem}\\n\\n\"\n        \"Candidate reasoning step:\\n\"\n        f\"\\\"\\\"\\\"\\n{reasoning_step}\\n\\\"\\\"\\\"\\n\\n\"\n        \"On a scale from 0 (completely incorrect) to 1 (perfectly correct), \"\n        \"rate how valid and useful this step is toward solving the problem. \"\n        \"Reply with only a number between 0 and 1.\"\n    )\n    # Call get_api_response(prompt), strip the result\n    resp = get_api_response(prompt).strip()\n    # Parse float(resp), fallback to 0.0 on ValueError, and return it\n    try:\n        return float(resp)\n    except ValueError:\n        return 0.0\n\n\ndef call_qwen_model_raw(prompt: str, step_num: int, temperature: float = 0.3):\n    \"\"\"\n    Sends a request to the local Qwen endpoint and returns the generated text\n    along with the average token log-probability and token count.\n    \"\"\"\n    max_tokens = {1: 500, 2: 800, 3: 1700}.get(step_num, 500)\n    url = \"http://localhost:8000/v1/chat/completions\"\n    payload = {\n        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature,\n        \"logprobs\": True,\n    }\n    # resp = requests.post(url, json=payload).json()\n    resp = requests.post(url, json=payload)\n    # Extract text = resp['choices'][0]['message']['content'].strip()\n    text = resp.json()['choices'][0]['message']['content'].strip()\n    # Gather token_logprobs from resp['choices'][*]['logprobs']['content'][*]['logprob']\n    token_logprobs = resp.json()['choices'][0]['logprobs']['content']\n    # Compute avg_logprob and num_token, then return (text, avg_logprob, num_token)\n    avg_logprob = sum([item['logprob'] for item in token_logprobs]) / len(token_logprobs) if token_logprobs else float('-inf')\n    num_token = len(token_logprobs)\n\n    return text, avg_logprob, num_token\n\n\nclass BeamCandidate:\n    def __init__(\n        self,\n        sequence: str,\n        cumulative_log_prob: float,\n        step_scores: List[float],\n        finished: bool,\n        num_token: int\n    ):\n        self.sequence = sequence\n        self.cumulative_log_prob = cumulative_log_prob\n        self.step_scores = step_scores\n        self.finished = finished\n        self.num_token = num_token\n\n    def __repr__(self):\n        return (\n            f\"BeamCandidate(score={self.cumulative_log_prob:.3f}, \"\n            f\"finished={self.finished}, sequence=[...])\"\n        )\n\n\ndef generate_reasoning_steps(\n    context: str,\n    step_num: int,\n    top_k: int,\n    use_logprob: bool = True,\n    problem: Optional[str] = None\n):\n    \"\"\"\n    Generate top_k candidate continuations for the current reasoning step.\n    \"\"\"\n    candidates = []\n\n    # Build `suffix` based on step_num (1=understand, 2=plan, 3=solve)\n    suffixes = {\n        1: \"\\n\\nFirst, I'll understand the problem by identifying what's given and what's being asked:\\n\",\n        2: \"\\n\\nNow, I'll plan my solution strategy by breaking down the steps:\\n\",\n        3: \"\\n\\nFinally, I'll solve step-by-step and provide the final answer in a boxed format:\\n\"\n    }\n    suffix = suffixes.get(step_num, \"\\n\\nNext, I'll continue my reasoning:\\n\")\n\n    for i in range(top_k):\n        # prompt = context + suffix\n        prompt = context + suffix\n        # output, avg_token_prob, num_token = call_qwen_model_raw(prompt, step_num)\n        output, avg_token_prob, num_token = call_qwen_model_raw(prompt, step_num)\n\n        # if use_logprob: score = avg_token_prob else: assert problem, score = score_with_gpt(problem, output)\n        if use_logprob:\n            score = avg_token_prob\n        else:\n            assert problem is not None, \"Problem must be provided when not using logprob\"\n            score = score_with_gpt(problem, output)\n\n        # finished = \"\\\\boxed{\" in output\n        finished = \"\\\\boxed{\" in output\n        # candidates.append((output.strip(), score, num_token, finished))\n        candidates.append((output.strip(), score, num_token, finished))\n\n    return candidates\n\n\ndef beam_search(\n    init_problem_prompt: str,\n    beam_width: int = 3,\n    max_steps: int = 3,\n    top_k: int = 2,\n    use_logprob: bool = True\n):\n    \"\"\"\n    Beam search over reasoning steps. If use_logprob=False, uses GPT verifier\n    to score each node instead of token log-prob.\n    \"\"\"\n    # Extract `problem` from init_problem_prompt\n    problem = init_problem_prompt.split(\"\\n\\n\")[0].strip()\n\n    # initial = BeamCandidate(sequence=init_problem_prompt, cumulative_log_prob=0.0, step_scores=[], finished=False, num_token=0)\n    initial = BeamCandidate(\n        sequence=init_problem_prompt,\n        cumulative_log_prob=0.0,\n        step_scores=[],\n        finished=False,\n        num_token=0\n    )\n    # beams = [initial]\n    beams = [initial]\n\n    for step in range(1, max_steps + 1):\n        new_beams = []\n\n        # For each cand in beams:\n        for cand in beams:\n            # If this candidate already has a boxed answer, keep it\n            if cand.finished:\n                new_beams.append(cand)\n                continue\n\n            # Generate new candidates from this state\n            step_cands = generate_reasoning_steps(cand.sequence, step, top_k, use_logprob, problem)\n\n            for text, score, n_tok, finished in step_cands:\n                seq = cand.sequence + \"\\n\" + text\n                total_tokens = cand.num_token + n_tok\n\n                # Calculate the new cumulative log probability\n                if total_tokens > 0:\n                    cum = ((cand.cumulative_log_prob * cand.num_token) + score * n_tok) / total_tokens\n                else:\n                    cum = score\n\n                new_beams.append(BeamCandidate(\n                    seq, cum, cand.step_scores + [score], finished, total_tokens\n                ))\n\n        # Sort candidates by score (higher is better) and keep only the top beam_width\n        new_beams.sort(key=lambda x: x.cumulative_log_prob, reverse=True)\n        beams = new_beams[:beam_width]\n\n        # If all candidates have finished, we can stop early\n        if all(beam.finished for beam in beams):\n            break\n\n    # Select the best finished candidate, or the best overall if none finished\n    finished = [b for b in beams if b.finished]\n    best = max(finished, key=lambda x: x.cumulative_log_prob) if finished else beams[0]\n    return best\n\n\ndef run_qwen_beam_search(\n    problem: str,\n    beam_width: int,\n    max_steps: int,\n    top_k: int,\n    log_level,\n    use_logprob: bool = True\n):\n    \"\"\"\n    Performs beam search and extracts the final boxed answer.\n    \"\"\"\n    prompt = f\"Consider this problem:\\n{problem}\\n\\nI'll solve this step by step.\"\n    best = beam_search(prompt, beam_width, max_steps, top_k, use_logprob)\n\n    if best.finished:\n        ans = extract_answer(best.sequence)\n        print(f\"\\nExtracted Final Answer: {ans}\")\n        return ans\n    else:\n        print(\"No final answer found.\")\n        return None\n",
   "metadata": {
    "id": "v2ZQr1A0WBD0",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T10:10:31.437450Z",
     "iopub.execute_input": "2025-05-23T10:10:31.438226Z",
     "iopub.status.idle": "2025-05-23T10:10:31.454541Z",
     "shell.execute_reply.started": "2025-05-23T10:10:31.438200Z",
     "shell.execute_reply": "2025-05-23T10:10:31.453844Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": "# Evaluate beam search\n* modify response generation part to evalute this method.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def evaluate_beam_search(use_logprob: bool = True):\n    \"\"\"\n    Evaluate beam search on MATH‑500, toggling between log‑prob scoring\n    and GPT/Gemini–based verification for each reasoning node.\n    \"\"\"\n    os.makedirs(\"results\", exist_ok=True)\n\n    suffix = \"logprob\" if use_logprob else \"gpt\"\n    results_file = f\"evaluation_results_math500_deepseek_beam_search_{suffix}.json\"\n\n    dataset = load_math500_dataset()\n    existing_results = load_existing_results(results_file)\n    processed_indexes = {r['index'] for r in existing_results}\n\n    cnt = 0\n    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n        if idx in processed_indexes or idx >= 30:\n            continue\n\n        problem_text   = item['problem']\n        correct_answer = extract_answer(item['solution'])\n\n        # Run beam search with the desired scoring method\n        response = run_qwen_beam_search(\n            problem     = problem_text,\n            beam_width  = 3,\n            max_steps   = 3,\n            top_k       = 2,\n            log_level   = 1,              # existing parameter\n            use_logprob = use_logprob     # True = token log‑prob, False = GPT verifier\n        )\n        predicted_answer = response\n\n        is_correct = compare_answers(correct_answer, predicted_answer)\n        save_result(results_file, {\n            \"index\": idx,\n            \"problem\": problem_text,\n            \"response\": response,\n            \"correct_answer\": correct_answer,\n            \"predicted_answer\": predicted_answer,\n            \"is_correct\": is_correct\n        })\n\n        if is_correct:\n            cnt += 1\n        print(f\"corrects: {cnt} / {idx+1}\")\n\n    final_results = load_existing_results(results_file)\n    analyze_results(final_results)\n\n",
   "metadata": {
    "id": "eR_TyvGMazTc",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T10:10:38.865332Z",
     "iopub.execute_input": "2025-05-23T10:10:38.865776Z",
     "iopub.status.idle": "2025-05-23T10:10:38.872063Z",
     "shell.execute_reply.started": "2025-05-23T10:10:38.865753Z",
     "shell.execute_reply": "2025-05-23T10:10:38.871517Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "# Example usage:\nevaluate_beam_search(use_logprob=True)   # pick by avg token log‑prob",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "sxMjVBf6bpiy",
    "outputId": "739ec144-b8e5-4403-feaf-f28e4b2f7eb9",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T10:10:46.761432Z",
     "iopub.execute_input": "2025-05-23T10:10:46.761998Z",
     "iopub.status.idle": "2025-05-23T11:37:47.172593Z",
     "shell.execute_reply.started": "2025-05-23T10:10:46.761976Z",
     "shell.execute_reply": "2025-05-23T11:37:47.171879Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "=== Results Summary ===\n",
    "\n",
    "Total problems: 30\n",
    "\n",
    "Correct answers: 16\n",
    "\n",
    "Accuracy: 53.33%\n",
    "\n",
    "Runtime: 1:26:59"
   ]
  },
  {
   "cell_type": "code",
   "source": "evaluate_beam_search(use_logprob=False)  # pick by GPT/Gemini verification",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T11:38:10.378963Z",
     "iopub.execute_input": "2025-05-23T11:38:10.379646Z",
     "iopub.status.idle": "2025-05-23T13:12:46.293236Z",
     "shell.execute_reply.started": "2025-05-23T11:38:10.379614Z",
     "shell.execute_reply": "2025-05-23T13:12:46.292550Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "=== Results Summary ===\n",
    "\n",
    "Total problems: 30\n",
    "\n",
    "Correct answers: 20\n",
    "\n",
    "Accuracy: 66.67%\n",
    "\n",
    "Runtime: 1:34:34"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Self-Refinement \n\nThis cell implements a self-refinement approach to solving math problems. Initially, it generates a solution using a fixed system prompt that enforces a step-by-step reasoning process and a final answer format enclosed in `\\boxed{}`. Then, through iterative feedback, the model is asked to analyze its own output and refine it if necessary. This loop ensures that the final answer is both correct and clearly formatted.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import re\nimport requests\n\nSYSTEM_PROMPT = '''You are solving mathematics problems.\n\nPlease think step by step.\n\nImportant: Always end your solution with the final answer in this format:\n\n\\\\[\n\\\\boxed{your_answer_here}\n\\\\]\n\nThe entire answer should be contained completely within the \\\\boxed{} command.'''\n\n\ndef generate_content(prompt: str) -> str:\n    \"\"\"\n    Sends `prompt` to the local Qwen endpoint and returns the generated text.\n    \"\"\"\n    url = \"http://localhost:8000/v1/chat/completions\"\n    payload = {\n        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n        \"max_tokens\": 1504,\n        \"temperature\": 0.3,\n    }\n    # Send HTTP POST to `url` with `payload`, parse the JSON response\n    response = requests.post(url, json=payload)\n    resp = response.json()\n    # Extract `content` from `resp['choices'][0]['message']['content']` and strip whitespace\n    content = resp['choices'][0]['message']['content'].strip()\n    # Return the resulting string\n    return content\n\n\ndef self_refine(problem: str, max_iter: int = 2) -> Optional[str]:\n    \"\"\"\n    Iteratively refines the model’s output on `problem` using feedback loops.\n    \"\"\"\n    # Build initial `prompt` by concatenating SYSTEM_PROMPT and `problem`\n    prompt = f\"{SYSTEM_PROMPT}\\n\\nProblem: {problem}\"\n    # Call `generate_content(prompt)` to get `current_output`\n    current_output = generate_content(prompt)\n\n    for iteration in range(max_iter):\n        # Construct `feedback_prompt`\n        feedback_prompt = (\n            f\"Imagine you are a proof reader.\\n\"\n            f\"PROBLEM:\\n{problem}\\n\\n\"\n            f\"CURRENT SOLUTION TO EVALUATE:\\n{current_output}\\n\\n\"\n            f\"Your job is to evaluate this solution. No need to solve the problem! just analyze and provide feedback for the current solution.\\n\"\n            f\"INSTRUCTIONS:\\n\"\n            f\"1. Check if the solution has CORRECT mathematics\\n\"\n            f\"2. Verify the final answer is inside \\\\boxed{{}} format\\n\"\n            f\"3. Respond using EXACTLY this template:\\n\\n\"\n            f\"[FEEDBACK]\\n\"\n            f\"Brief critique of mathematical correctness\\n\\n\"\n            f\"[REFINEMENT_NEEDED]\\n\"\n            f\"yes OR no (only one word)\\n\\n\"\n        )\n\n        # Call `generate_content(feedback_prompt)` → `feedback_response`\n        feedback_response = generate_content(feedback_prompt)\n\n        # Determine `refinement_needed` (default True, set False if flag is \"no\")\n        refinement_needed = True\n\n        # Pattern 1: Standard format with headers\n        match = re.search(r\"\\[(?i:feedback)\\](.*?)\\[(?i:refinement_needed)\\](.*)\",\n                         feedback_response, re.DOTALL)\n        if match:\n            feedback = match.group(1).strip()\n            refinement_flag = match.group(2).strip().lower()\n\n            # Check if refinement is needed\n            if refinement_flag in [\"no\", \"no.\"]:\n                refinement_needed = False\n        else:\n            # Pattern 2: Look for yes/no statements about refinement\n            no_patterns = [\n                r\"refinement(?:\\s+is)?\\s+(?:not|NOT)\\s+needed\",\n                r\"no refinement(?:\\s+is)?\\s+needed\",\n                r\"solution is correct\",\n                r\"\\[refinement_needed\\]:\\s*no\",\n                r\"\\[refinement_needed\\]\\s*no\"\n            ]\n\n            for pattern in no_patterns:\n                if re.search(pattern, feedback_response, re.IGNORECASE):\n                    refinement_needed = False\n                    break\n\n            # If regex doesn't match, use the whole response as feedback\n            feedback = feedback_response\n\n        # If `not refinement_needed`: break out of loop\n        if not refinement_needed:\n            break\n\n        # Build `refine_prompt`\n        refine_prompt = (\n            f\"Consider this initial prompt and problem: \\n\"\n            f\"\\\"{prompt}\\\" \\n \\n\"\n            f\"Here is my current solution: \\n\"\n            f\"\\\"{current_output}\\\" \\n\\n\"\n            f\"Feedback on the solution: \\n\"\n            f\"\\\"{feedback}\\\" \\n\\n\"\n            f\"Please provide an improved solution based on this feedback. \\n\"\n            f\"Remember to follow the step-by-step approach and end with the final answer in a boxed format.\"\n        )\n\n        # Call `generate_content(refine_prompt)` → `refined_output`\n        refined_output = generate_content(refine_prompt)\n\n        # Check if the solution has changed\n        if refined_output.strip() == current_output.strip():\n            break\n\n        current_output = refined_output\n    \n    # Use `extract_answer(current_output)` to get the final boxed answer\n    answer = extract_answer(current_output)\n    # Return that answer (or None if no boxed answer found)\n    return answer\n",
   "metadata": {
    "id": "fAYp7rCbiJQa",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T13:13:25.783427Z",
     "iopub.execute_input": "2025-05-23T13:13:25.783724Z",
     "iopub.status.idle": "2025-05-23T13:13:25.793082Z",
     "shell.execute_reply.started": "2025-05-23T13:13:25.783703Z",
     "shell.execute_reply": "2025-05-23T13:13:25.792299Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": "# Evaluate self refiner\n* modify response generation part to evalute this method.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def evaluate_self_refiner():\n    os.makedirs(\"results\", exist_ok=True)\n    results_file = \"evaluation_results_math500_deepseek_self_refiner.json\"\n    dataset = load_math500_dataset()\n    existing_results = load_existing_results(results_file)\n    processed_indexes = {result['index'] for result in existing_results}\n    cnt = 0\n    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n        if idx in processed_indexes :\n            continue\n        if idx >= 30:\n          break\n        problem_text = item['problem']\n        correct_answer = extract_answer(item['solution'])\n        ##########################################################\n        response = self_refine(problem_text, 3)\n        predicted_answer = response\n        ##########################################################\n        is_correct = compare_answers(correct_answer, predicted_answer)\n        result = {\n            \"index\": idx,\n            \"problem\": problem_text,\n            \"response\": response,\n            \"correct_answer\": correct_answer,\n            \"predicted_answer\": predicted_answer,\n            \"is_correct\": is_correct\n        }\n        save_result(results_file, result)\n        if is_correct:\n          cnt += 1\n        print(f\"corrects :  {cnt} idx: {idx}\")\n    final_results = load_existing_results(results_file)\n    analyze_results(final_results)",
   "metadata": {
    "id": "ln5I9pg_jN7Z",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T13:13:34.145926Z",
     "iopub.execute_input": "2025-05-23T13:13:34.146588Z",
     "iopub.status.idle": "2025-05-23T13:13:34.152095Z",
     "shell.execute_reply.started": "2025-05-23T13:13:34.146566Z",
     "shell.execute_reply": "2025-05-23T13:13:34.151472Z"
    }
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": "evaluate_self_refiner()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "uNAg9wa9jlSu",
    "outputId": "64033f38-3340-4729-be0e-70dd0d792705",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T13:13:36.959634Z",
     "iopub.execute_input": "2025-05-23T13:13:36.960159Z",
     "iopub.status.idle": "2025-05-23T13:54:54.049733Z",
     "shell.execute_reply.started": "2025-05-23T13:13:36.960138Z",
     "shell.execute_reply": "2025-05-23T13:54:54.048927Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "=== Results Summary ===\n",
    "\n",
    "Total problems: 30\n",
    "\n",
    "Correct answers: 15\n",
    "\n",
    "Accuracy: 50.00%\n",
    "\n",
    "Runtime: 41:16"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 🚀 Overview (Part 2): Implementing A*, Monte Carlo Tree Search (MCTS), and Tree of Thoughts (ToT) \n\nOk so now every thing is ready to start part 2, This part aims to explore three sophisticated search and reasoning algorithms—**A\\***, **Monte Carlo Tree Search (MCTS)**, and **Tree of Thoughts (ToT)**—to solve challenging mathematical problems, specifically using the MATH-500 dataset and an LLM (Language Model). Before diving into the implementation, we provide a comprehensive overview of each algorithm, highlighting their core mechanisms, practical considerations, and potential challenges.\n\n---\n\n## 🌟 1. A* Search Algorithm\n\n**A*** is an informed search algorithm designed for efficiently finding the shortest path or optimal solution in a search space using heuristics.\n\n### 🔹 Core Principles:\n- **Best-first search:** Expands nodes based on a cost function, \\(f(n)\\), prioritizing paths that seem closer to a goal.\n- **Heuristic evaluation:** Uses a heuristic function \\(h(n)\\) to estimate the cost from the current node to the goal.\n\n### 🔹 Components:\n- **Cost function \\(f(n) = g(n) + h(n)\\)**, where:\n  - \\(g(n)\\): Actual cost from start node to node \\(n\\).\n  - \\(h(n)\\): Estimated cost from node \\(n\\) to the goal (heuristic).\n\n### 🔹 Practical Considerations:\n- Heuristic function must be **efficient** and **accurate**.\n- A good heuristic drastically reduces computation time and search complexity.\n\n### ⚠️ Challenges in Implementation:\n- **Designing an effective heuristic:**\n  - Challenge to accurately estimate \"distance\" from partial reasoning steps to the solution.\n  - Requires leveraging language models to score plausibility.\n- **Computational efficiency:**\n  - Heuristic evaluation via LLM queries could be computationally costly if not managed carefully.\n- **Admissibility and consistency:**\n  - Ideally, heuristic must be admissible (never overestimates the true cost) to guarantee optimality.\n\n---\n\n## 🎲 2. Monte Carlo Tree Search (MCTS)\n\n**MCTS** is a probabilistic algorithm widely used in decision-making scenarios, especially effective in complex problems with uncertain outcomes, such as mathematical reasoning guided by language models.\n\n### 🔹 Core Principles:\nMCTS explores decision trees using **randomized simulations** (rollouts) and statistical sampling.\n\n**Four main steps in MCTS**:\n1. **Selection**: Uses UCT (Upper Confidence Bound) to balance exploration and exploitation.\n2. **Expansion**: Adds new unexplored nodes to the tree.\n3. **Simulation**: Conducts rollouts from newly expanded nodes to estimate potential outcomes.\n4. **Backpropagation**: Updates statistical measures based on simulation results.\n\n### 🔹 Components:\n- **UCT formula** for selection:\n  $$\n  \\text{UCT} = \\frac{w_i}{n_i} + C\\sqrt{\\frac{\\ln N}{n_i}}\n  $$\n  - \\(w_i\\): Total rewards.\n  - \\(n_i\\): Visits to node \\(i\\).\n  - \\(N\\): Visits to parent node.\n  - \\(C\\): Exploration constant (usually \\(\\sqrt{2}\\)).\n\n- **Rollout (simulation)**:\n  - Typically involves letting the LLM complete reasoning steps to the end, evaluating correctness.\n\n### 🔹 Practical Considerations:\n- Effective rollout policies significantly impact accuracy and efficiency.\n- Balance exploration (testing new reasoning paths) and exploitation (refining known good solutions).\n\n### ⚠️ Challenges in Implementation:\n- **Computational overhead**:\n  - Running multiple LLM-based rollouts per node can be slow and computationally expensive.\n- **Optimal parameter tuning**:\n  - Choosing the exploration constant \\(C\\) and number of simulations impacts performance significantly.\n- **Quality of simulation outcomes**:\n  - Poor rollout outcomes (random or inaccurate completions) can misguide the search tree.\n\n---\n\n## 🌳 3. Tree of Thoughts (ToT)\n\n**Tree of Thoughts (ToT)** is specifically designed for structured reasoning tasks with language models, extending their capabilities through explicit evaluation and pruning of reasoning paths.\n\n### 🔹 Core Principles:\n- Generate multiple candidate \"thoughts\" (reasoning paths).\n- Evaluate each thought explicitly (often through LLM-based scoring or consistency checking).\n- Iteratively prune weaker reasoning paths, keeping the most promising solutions.\n\n### 🔹 Components:\n- **Thought generation**: Multiple candidate reasoning steps generated at each node.\n- **Thought evaluation**: Explicit scoring (via LLMs) to judge mathematical correctness or plausibility.\n- **Pruning**: Remove less promising reasoning branches based on evaluation.\n\n### 🔹 Practical Considerations:\n- Explicit node evaluation adds a structured layer of reasoning not present in simpler methods.\n- Allows LLMs to reason more deliberately by systematically exploring and eliminating alternatives.\n\n### ⚠️ Challenges in Implementation:\n- **Evaluation complexity**:\n  - Frequent explicit evaluations by LLM can slow the process.\n  - Requires efficient prompting and scoring techniques.\n- **Self-consistency**:\n  - Maintaining logical consistency across multiple branches can be difficult, especially for complex math problems.\n- **Scaling**:\n  - Managing multiple branches of reasoning can quickly become computationally expensive without careful control.\n\n---\n\n\n**Next, we will begin the practical implementation step-by-step.**\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Implementing Tree of Thoughts (ToT)\n\nBefore writing any code, it’s essential to map out the steps and functions we need for our Tree of Thoughts (ToT) implementation. In ToT, we aren’t just following one linear chain of reasoning but instead generating a tree of candidate reasoning paths (“thoughts”), evaluating them, and then expanding the most promising ones. We can leverage our existing helper functions (such as those for extracting and normalizing answers) as part of the evaluation process.\n\nBelow is an outline of the steps and functions we’ll need:\n\n---\n\n## 1. **Node Representation**\n\nWe need a way to represent each node in our reasoning tree. A node could include:\n- **Current reasoning text**: The partial solution or thought generated so far.\n- **Candidate thoughts**: A list of potential next steps (children nodes).\n- **Evaluation score**: A score indicating how promising the node is (based on plausibility or correctness).\n- **Metadata**: Such as depth in the tree or a reference to the parent node.\n\n**Potential Functions/Classes:**\n- `class Node`: A class that encapsulates the above properties.\n- `add_child(self, child_node)`: A method to attach a new candidate thought.\n\n---\n\n## 2. **Candidate Thought Generation**\n\nThis function will use the LLM (via our `get_llm_response` function) to generate multiple candidate reasoning steps given a node's current state. \n\n**Key Points:**\n- The prompt should be carefully crafted to ask the LLM for alternative reasoning steps.\n- We can use techniques such as few-shot prompting to guide the LLM in generating diverse thoughts.\n\n**Potential Function:**\n- `def generate_candidate_thoughts(node: Node, num_candidates: int) -> List[str]:`\n  - This function takes the current reasoning state from `node` and returns a list of candidate thoughts (as strings).\n\n---\n\n## 3. **Candidate Thought Evaluation**\n\nOnce we have multiple candidate thoughts, we need to score them. The evaluation could be based on:\n- **Model’s self-assessment:** Ask the LLM to rate each candidate on a scale (e.g., 1 to 10) for mathematical plausibility or correctness.\n- **Heuristics based on helper functions:** Use helper functions like `extract_answer` and `normalize_answer` to check whether a candidate thought moves closer to a correct answer or simplifies the expression.\n\n**Potential Function:**\n- `def evaluate_candidate_thought(candidate: str) -> float:`\n  - This function might prompt the LLM with the candidate reasoning step, asking, “How plausible or correct is this step?” and return a numeric score.\n  - Alternatively, it might combine an LLM score with our own heuristic checks.\n\n---\n\n## 4. **Pruning and Selection**\n\nAfter evaluating the candidates, we must select the most promising ones for further expansion. Pruning involves:\n- Ranking candidate thoughts by their evaluation score.\n- Keeping only the top N candidates (to control the tree size).\n\n**Potential Function:**\n- `def select_best_candidates(candidates: List[str], scores: List[float], top_n: int) -> List[str]:`\n  - This function will combine the candidate list and their scores to select the best ones for further exploration.\n\n---\n\n## 5. **Node Expansion**\n\nFor each node, the process is:\n1. **Generate candidate thoughts** using the generation function.\n2. **Evaluate each candidate** using the evaluation function.\n3. **Select the best candidate(s)** using the pruning/selection function.\n4. **Expand the tree** by creating child nodes for each selected candidate.\n\n**Potential Function:**\n- `def expand_node(node: Node, num_candidates: int, top_n: int) -> None:`\n  - This function integrates candidate generation, evaluation, and pruning to add new child nodes to the given `node`.\n\n---\n\n## 6. **Stopping Criteria**\n\nWe need clear criteria for when to stop expanding the tree:\n- **Complete solution found:** When a node contains a complete solution (e.g., using `extract_answer` to verify that a boxed answer exists).\n- **Depth or resource limits:** When a maximum depth is reached or computational resources are constrained.\n\n**Potential Function:**\n- `def is_solution(node: Node) -> bool:`\n  - This function checks if a node’s reasoning contains a valid, complete answer.\n- `def stop_expansion(node: Node, max_depth: int) -> bool:`\n  - This function checks if the node has reached the maximum allowed depth.\n\n---\n\n## 7. **Integration with Helper Functions**\n\nOur previous helper functions play a crucial role in the ToT implementation:\n- **Extracting and normalizing answers:**  \n  Use `extract_answer` and `normalize_answer` to interpret candidate outputs and compare them against the expected solution.\n- **Comparison functions:**  \n  Use `compare_answers` to help decide if a candidate thought is moving in the right direction.\n- **LLM response function:**  \n  `get_llm_response` is used both for generating candidate thoughts and possibly for scoring them.\n\n---\n\n## 8. **Overall ToT Flow**\n\nPutting it all together, here is an outline of the overall ToT process:\n1. **Initialize the root node** with the initial problem statement.\n2. **While the stopping criteria are not met:**\n   - For the current node, generate candidate thoughts.\n   - Evaluate each candidate.\n   - Select and expand the best candidates to form new child nodes.\n3. **Once a candidate thought leads to a complete solution:**\n   - Use helper functions to verify correctness.\n   - Return or record the successful reasoning path.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import re\nimport time\nfrom typing import List, Optional\n\ndef score_with_gpt(problem: str, candidate: str) -> float:\n    \"\"\"\n    Ask a high‑quality LLM (via get_api_response) to rate the given\n    final answer on a scale from 1 (poor) to 10 (excellent).\n    \"\"\"\n    # build `eval_prompt` string using `problem` and `candidate`\n    eval_prompt = (\n        f\"You are a math problem evaluator.\\n\\n\"\n        f\"Problem:\\n{problem}\\n\\n\"\n        f\"Candidate solution step:\\n{candidate}\\n\\n\"\n        f\"Rate this solution step from 1-10 based on these criteria:\\n\"\n        f\"- Mathematical correctness\\n\"\n        f\"- Relevance to solving the problem\\n\"\n        f\"- Progress toward the final answer\\n\"\n        f\"Respond with only a number between 1 and 10.\"\n    )\n    resp = get_api_response(eval_prompt).strip()\n\n    # use `re.search` to extract the first numeric score from `resp`\n    match = re.search(r'(\\d+(?:\\.\\d+)?)', resp)\n\n    # return float(match) or 0.0 on failure\n    return float(match.group(1)) if match else 0.0\n\n\nclass Node:\n    def __init__(self, state: str, depth: int = 0, parent: Optional['Node'] = None):\n        self.state = state\n        self.depth = depth\n        self.parent = parent\n        self.children: List[Node] = []      # easy: list to hold child nodes\n        self.score: Optional[float] = None  # will be set once evaluated\n\n    def add_child(self, child_node: 'Node'):\n        self.children.append(child_node)\n\n    def __repr__(self):\n        # easy: show depth, score, and a truncated preview of `state`\n        preview = self.state if len(self.state) < 50 else self.state[:47] + \"...\"\n        return f\"Node(depth={self.depth}, score={self.score}, state='{preview}')\"\n\n    def get_complete_reasoning_path(self):\n        \"\"\"\n        Returns the complete reasoning path from the root to this node as a string.\n        \"\"\"\n        path = []\n        current_node = self\n        while current_node is not None:\n            path.append(current_node.state)\n            current_node = current_node.parent\n        return \"\\n\".join(reversed(path))\n\n\nclass TreeOfThoughts:\n    def __init__(\n        self,\n        num_candidates: int = 3,\n        top_n: int = 2,\n        max_depth: int = 3,\n        verbose: bool = False\n    ):\n        self.num_candidates = num_candidates\n        self.top_n = top_n\n        self.max_depth = max_depth\n        self.verbose = verbose\n        self.root_state: Optional[str] = None\n\n    def generate_candidate_thoughts(self, node: Node) -> List[str]:\n        \"\"\"\n        Prompt the LLM to generate candidate thoughts based on the current node's state.\n        \"\"\"\n        # compose prompt with node.state\n        prompt = (\n            f\"You are a mathematician solving a step-by-step problem.\\n\\n\"\n            f\"PROBLEM AND CURRENT SOLUTION:\\n{node.get_complete_reasoning_path()}\\n\\n\"\n            f\"TASK: Generate exactly {self.num_candidates} different next steps to continue this solution.\\n\\n\"\n            f\"REQUIREMENTS:\\n\"\n            f\"- Each step must be NEW and continue directly from where the solution stopped\\n\"\n            f\"- Keep each step brief (2-3 sentences maximum)\\n\"\n            f\"- Make sure steps are mathematically diverse (different approaches)\\n\"\n            f\"- Do NOT repeat any previous work or reasoning\\n\"\n            f\"- Do NOT provide complete solutions\\n\\n\"\n            f\"- If a step lead to the answer, provide the final answer in this format: \\\\boxed{{your answer here}}.\"\n            f\"FORMAT: Numbered list with each step on a new line\\n\"\n            f\"1. [first suggestion]\\n\"\n            f\"2. [second suggestion]\\n\"\n            f\"...\\n\\n\"\n        )\n        response = get_llm_response(prompt, 1900)\n\n        # Extract content after </think> tag\n        parts = response.split(\"</think>\", 1)\n        if len(parts) > 1:\n            # Take everything after the thinking tag\n            post_thinking = parts[1].strip()\n            candidates = [line.strip() for line in post_thinking.split('\\n') if line.strip()]\n        else:\n            # If no </think> tag, just split by lines\n            candidates = [line.strip() for line in response.split('\\n') if line.strip()]\n\n        # Remove numbering prefixes from the candidates (like \"1.\", \"2)\", \"(3)\", etc.)\n        candidates = [re.sub(r'^\\s*(?:\\d+[.):]\\s*|\\(\\d+\\)\\s*)', '', candidate).strip() for candidate in candidates]\n\n        # If we don't have enough candidates, add lines from the thinking phase\n        if len(candidates) < self.num_candidates and len(parts) > 1:\n            thinking_lines = [line.strip() for line in parts[0].split('\\n') if line.strip()]\n            candidates.extend(thinking_lines[:self.num_candidates - len(candidates)])\n\n        # Take only the required number of candidates\n        candidates = candidates[:self.num_candidates]\n\n        return candidates\n\n    def evaluate_candidate_thought(self, candidate: str) -> float:\n        # simple wrapper around score_with_gpt\n        assert self.root_state is not None\n        return score_with_gpt(self.root_state, candidate)\n\n    def select_best_candidates(self, candidates: List[str]) -> List[str]:\n        scored_candidates = []\n        for cand in candidates:\n            score = self.evaluate_candidate_thought(cand)\n            scored_candidates.append((cand, score))\n            time.sleep(0.5)  # Prevent rate limiting\n\n        # Sort by score in descending order\n        scored_candidates.sort(key=lambda x: x[1], reverse=True)\n\n        # Return the top N candidates\n        return [c[0] for c in scored_candidates[:self.top_n]]\n\n    def expand_node(self, node: Node) -> None:\n        if self.verbose:\n            print(f\"\\nExpanding depth {node.depth} state:\\n{node.state}\\n\")\n\n        raw = self.generate_candidate_thoughts(node)\n        best = self.select_best_candidates(raw)\n\n        for ans in best:\n            child = Node(state=ans, depth=node.depth + 1, parent=node)\n            child.score = self.evaluate_candidate_thought(ans)  # evaluate the child\n\n            # if not boxed, retry up to 3 times with a strict prompt\n            attempts = 0\n            while not extract_answer(child.state) and attempts < 3 and node.depth >= self.max_depth - 1:\n                retry_prompt = (\n                    f\"Based on your work:\\n{child.get_complete_reasoning_path()}\\n\\n\"\n                    f\"Please provide a final answer in this format: \\\\boxed{{your answer here}}\"\n                )\n                retry_response = get_llm_response(retry_prompt, 1900)\n                child.state = retry_response\n                attempts += 1\n\n            node.add_child(child)\n            if self.verbose:\n                print(f\"Added child: {child}\")\n\n    def is_solution(self, node: Node) -> bool:\n        # return True if `extract_answer(node.state)` yields non-empty\n        return extract_answer(node.state) is not None\n\n    def stop_expansion(self, node: Node) -> bool:\n        return node.depth >= self.max_depth\n\n    def search(self, root_state: str) -> Node:\n        \"\"\"\n        Build the tree until solutions found or max depth reached.\n        \"\"\"\n        self.root_state = root_state\n        root = Node(state=root_state, depth=0)\n        frontier = [root]\n        while frontier:\n            node = frontier.pop(0)\n            if self.is_solution(node) or self.stop_expansion(node):\n                continue\n            self.expand_node(node)\n            frontier.extend(node.children)\n        return root\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T13:55:14.993505Z",
     "iopub.execute_input": "2025-05-23T13:55:14.993788Z",
     "iopub.status.idle": "2025-05-23T13:55:15.012894Z",
     "shell.execute_reply.started": "2025-05-23T13:55:14.993767Z",
     "shell.execute_reply": "2025-05-23T13:55:15.012297Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": "# Testing the Tree of Thoughts Method with Minimal Hyperparameters\n\nThis cell demonstrates a test run of the Tree of Thoughts (ToT) framework using minimal hyperparameters. The goal is to ensure that a complete final answer (in the format `\\boxed{...}`) is extracted from the model's output.\n\n**Key Steps:**\n\n- **Instantiate TOT:**  \n  The TOT instance is created with `num_candidates=1`, `top_n=1`, and `max_depth=1` in verbose mode. This minimal setup is used for quick testing.\n\n- **Run the Search:**  \n  The TOT search is executed on the sample problem:  \n  *\"Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)\"*\n\n- **Print the TOT Tree:**  \n  A recursive function (`print_tree`) prints the entire search tree, allowing inspection of each node's state and depth.\n\n- **Extract Final Answer:**  \n  All nodes in the tree are collected. If a node is found that contains a final answer (determined via the helper function `extract_answer`), then the node's state is overwritten to display only the final answer in the correct boxed format.  \n  If no such node is found, a fallback prompt forces the model to output the final answer exclusively.\n\nThis setup helps verify that the TOT framework correctly isolates and formats the final answer, ensuring the result is comparable to the expected output.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Test the TreeOfThoughts with minimal settings.\n# Generation of candidate answers still uses your primary LLM via get_llm_response,\n# while evaluation/scoring uses only the GPT/Gemini verifier.\n\ntot = TreeOfThoughts(num_candidates=2,\n                     top_n=1,\n                     max_depth=3,\n                     verbose=True)\n\ninitial_problem = \"Solve the integral: \\\\( \\\\int_0^1 x^2 \\\\, dx \\\\)\"\ntot_tree = tot.search(initial_problem)\n\n# Helper to print the whole tree\ndef print_tree(node: Node, indent: str = \"\"):\n    print(indent + repr(node))\n    for child in node.children:\n        print_tree(child, indent + \"  \")\n\nprint_tree(tot_tree)\n\n# Collect all nodes and find the first complete solution\ndef collect_all_nodes(node: Node) -> List[Node]:\n    nodes = [node]\n    for child in node.children:\n        nodes.extend(collect_all_nodes(child))\n    return nodes\n\nall_nodes = collect_all_nodes(tot_tree)\nsolution_node = next((n for n in all_nodes if tot.is_solution(n)), None)\n\nprint(\"---\")\nif solution_node:\n    final = extract_answer(solution_node.state)\n    solution_node.state = f\"\\\\boxed{{{final}}}\"\n    print(\"\\nFinal Answer Found:\")\n    print(solution_node.state)\nelse:\n    # If no solution node was produced, force a final answer via GPT verifier\n    fallback_prompt = (\n        f\"Based on the problem: \\\"{initial_problem}\\\", please provide ONLY your final answer \"\n        \"in the exact format \\\\boxed{...}.\"\n    )\n    forced = get_api_response(fallback_prompt).strip()\n    final = extract_answer(forced) or forced\n    forced = f\"\\\\boxed{{{final}}}\"\n    print(\"\\nNo complete final answer was found. Forcing final answer:\")\n    print(forced)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T13:55:22.729967Z",
     "iopub.execute_input": "2025-05-23T13:55:22.730479Z",
     "iopub.status.idle": "2025-05-23T13:57:03.971648Z",
     "shell.execute_reply.started": "2025-05-23T13:55:22.730452Z",
     "shell.execute_reply": "2025-05-23T13:57:03.970950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\nExpanding depth 0 state:\nSolve the integral: \\( \\int_0^1 x^2 \\, dx \\)\n\nAdded child: Node(depth=1, score=10.0, state='Use the antiderivative of x squared, which is (...')\n\nExpanding depth 1 state:\nUse the antiderivative of x squared, which is (x³)/3, and evaluate from 0 to 1 to get 1/3.\n\nAdded child: Node(depth=2, score=10.0, state='Subtract the value of the antiderivative at the...')\n\nExpanding depth 2 state:\nSubtract the value of the antiderivative at the lower limit 0, which is 0.\n\nAdded child: Node(depth=3, score=10.0, state='Okay, so I need to solve the integral of x squa...')\nNode(depth=0, score=None, state='Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)')\n  Node(depth=1, score=10.0, state='Use the antiderivative of x squared, which is (...')\n    Node(depth=2, score=10.0, state='Subtract the value of the antiderivative at the...')\n      Node(depth=3, score=10.0, state='Okay, so I need to solve the integral of x squa...')\n---\n\nFinal Answer Found:\n\\boxed{\\dfrac{1}{3}}\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": "# Evaluation of the Tree of Thoughts (ToT) Method on the Math500 Dataset\n\nThis evaluation framework is designed to test the ToT method on a subset of the Math500 dataset. It provides flexibility in hyperparameter configuration and is set up for both debugging with detailed output and large-scale evaluation. The framework saves results and summarizes key metrics, and it includes mechanisms to force the model to output a final answer in the expected format.\n\n## Key Features\n\n- **Unique Results File:**  \n  Uses a dedicated results file (e.g., `evaluation_results_tot_test.json`) to store evaluation data. The file is cleared at the beginning of each run to prevent interference from previous evaluations.\n\n- **Configurable Hyperparameters:**  \n  You can adjust parameters such as:\n  - `num_candidates`: Number of candidate final answers generated per node.\n  - `top_n`: Number of top candidates selected for expanding the tree.\n  - `max_depth`: Maximum depth of the search tree.\n  \n  These parameters enable you to test different reasoning strategies and trade-offs between search breadth and depth.\n\n- **Sample Selection:**  \n  The `max_samples` parameter controls the number of problems from the dataset to evaluate. This allows you to start by testing on a single sample and then scale up to 100 or even more problems as desired.\n\n- **Debug and Fallback Mechanisms:**  \n  - The framework prints the entire TOT tree for each problem to facilitate inspection.\n  - If no node contains a complete final answer (i.e., a final answer wrapped in `\\boxed{...}`), a fallback prompt is issued to force the model to output the final answer.\n  - Detailed debug outputs help track the process and diagnose any issues in final answer extraction.\n\n- **Final Answer Extraction:**  \n  After processing the tree, the system extracts just the final answer from the model's output (using a helper function like `extract_answer`), ensuring that only the final answer (and no chain-of-thought explanations) is compared against the correct answer.\n\n- **Result Saving and Analysis:**  \n  The framework saves each problem’s evaluation (including problem text, responses, and correctness) and produces a summary report that includes metrics like total problems, correct answers, and overall accuracy.\n\n## Encouragement for Further Improvement\n\n**Prompt Engineering & Fallback Strategies:**  \nThe current method forces the model to provide a final answer using a fallback prompt when the initial generation does not meet the required format. While this approach works, it is not perfect:\n- **Prompt Tuning:** Experiment with different wording and structure in the prompts. For example, try different phrasings that emphasize \"ONLY your final answer\" and \"no additional explanations\" to see if the model can be nudged into generating a cleaner response.\n- **Iterative Refinement:** Consider implementing iterative prompt refinement mechanisms or leveraging additional post-processing steps to filter out unwanted chain-of-thought text.\n- **Open Research Problem:** The issue of controlling a language model’s output to include only the final answer (and not intermediate reasoning) is an active area of research. There is significant potential to explore improved strategies that maintain reasoning power while enforcing output constraints.\n\n**Scalability:**  \nOnce you have fine-tuned the hyperparameters and the prompting strategy on a small sample, encourage testing over a larger set (such as 100 or more problems). Evaluating on a larger dataset can help identify trends and potential improvements that might not be apparent on a smaller scale.\n\n**Experiment and Innovate:**  \nDo not hesitate to modify the prompts, fallback mechanisms, and even the underlying structure of the TOT class. Every change you experiment with might lead to better results and a deeper understanding of how to steer the model toward producing just the final answer. Your experimentation is key to achieving a more robust and reliable evaluation system.\n\nThis framework is designed to be flexible—feel free to tweak the parameters and strategies to suit your research or production needs, and continue to iterate toward better performance!\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nfrom tqdm import tqdm\n\ndef evaluate_tot(max_samples: int = 1):\n    \"\"\"\n    Evaluate the Tree of Thoughts (ToT) method on a subset of the Math500 dataset.\n    Uses GPT/Gemini exclusively for verification and fallback.\n    Assumes the existence of helper functions:\n      - load_math500_dataset()\n      - extract_answer(solution_text)\n      - compare_answers(correct_answer, predicted_answer)\n      - save_result(results_file, result_dict)\n      - load_existing_results(results_file)\n      - analyze_results(results_list)\n      - get_llm_response(prompt)     # for initial candidate generation\n      - get_api_response(prompt)     # for GPT/Gemini–based verification\n    \"\"\"\n    os.makedirs(\"results\", exist_ok=True)\n    results_file = \"results/evaluation_results_tot_test.json\"\n    if os.path.exists(results_file):\n        os.remove(results_file)\n\n    dataset = load_math500_dataset()\n    existing = load_existing_results(results_file)\n    processed = {r['index'] for r in existing}\n\n    tot = TreeOfThoughts(num_candidates=3, top_n=1, max_depth=3, verbose=True)\n    correct_count = 0\n    evaluated = 0\n\n    def collect_all_nodes(node):\n        nodes = [node]\n        for child in node.children:\n            nodes.extend(collect_all_nodes(child))\n        return nodes\n\n    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n        if idx in processed or evaluated >= max_samples:\n            continue\n\n        problem_text = item['problem']\n        correct_answer = extract_answer(item['solution'])\n\n        # Run the Tree‑of‑Thoughts search\n        tot_tree = tot.search(problem_text)\n\n        # Debug print of the entire tree\n        print(f\"\\n--- DEBUG: Full TOT tree for problem index {idx} ---\")\n        def print_tree(node, indent=\"\"):\n            print(indent + repr(node))\n            for c in node.children:\n                print_tree(c, indent + \"  \")\n        print_tree(tot_tree)\n        print(\"--- End of TOT tree ---\\n\")\n\n        # Find first node with a boxed answer\n        all_nodes = collect_all_nodes(tot_tree)\n        solution_node = next((n for n in all_nodes if tot.is_solution(n)), None)\n\n        if solution_node:\n            response = solution_node.state\n            print(\"DEBUG: Found solution node:\", response)\n        else:\n            # Fallback: ask GPT/Gemini directly for final answer\n            print(\"DEBUG: No solution node found. Using GPT fallback.\")\n            fallback_prompt = (\n                f\"Based on the problem: \\\"{problem_text}\\\", provide ONLY your final answer \"\n                \"in the exact format \\\\boxed{<final answer>}. Do not include any chain-of-thought.\"\n            )\n            response = get_api_response(fallback_prompt)\n            print(\"DEBUG: Fallback response:\", response)\n\n        predicted = extract_answer(response) or \"\"\n        if not predicted:\n            print(\"DEBUG: predicted_answer is empty. Raw response was:\\n\", response)\n\n        is_correct = compare_answers(correct_answer, predicted)\n        save_result(results_file, {\n            \"index\": idx,\n            \"problem\": problem_text,\n            \"response\": response,\n            \"correct_answer\": correct_answer,\n            \"predicted_answer\": predicted,\n            \"is_correct\": is_correct\n        })\n\n        if is_correct:\n            correct_count += 1\n        evaluated += 1\n        print(f\"Correct: {correct_count}/{evaluated} | Index: {idx}\")\n\n    final = load_existing_results(results_file)\n    analyze_results(final)\n\n# Example: test a single sample\nevaluate_tot(max_samples=30)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T13:57:18.681042Z",
     "iopub.execute_input": "2025-05-23T13:57:18.681432Z",
     "iopub.status.idle": "2025-05-23T15:07:56.066564Z",
     "shell.execute_reply.started": "2025-05-23T13:57:18.681409Z",
     "shell.execute_reply": "2025-05-23T15:07:56.065888Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "=== Results Summary ===\n",
    "\n",
    "Total problems: 30\n",
    "\n",
    "Correct answers: 16\n",
    "\n",
    "Accuracy: 53.33%\n",
    "\n",
    "Runtime: 1:10:36"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# A* Search Algorithm for Mathematical Reasoning\n\nThe A* (A-star) search algorithm is a popular informed search method that combines both actual cost and an estimated cost to reach the goal. When applied to mathematical problem-solving with language models, each node in the search tree represents a partial reasoning process. The goal is to guide the search toward the correct final answer efficiently.\n\n## Core Concepts\n\n- **Nodes and States:**  \n  Each node represents a state in the reasoning process—a partial solution or chain-of-thought step. The root node is the initial problem, and child nodes represent possible next steps in reasoning.\n\n- **Cost Function (g(n)):**  \n  This function measures the cost accumulated from the start node to the current node. In our context, it might represent the complexity or length of the reasoning chain so far.\n\n- **Heuristic Function (h(n)):**  \n  A heuristic estimates the cost (or “distance”) from the current node to the goal (a complete final answer). For mathematical reasoning, this could be designed to reflect how promising the current partial solution is—possibly by prompting the LLM to provide a confidence or plausibility score.\n\n- **Evaluation Function (f(n)):**  \n  A* uses the function:\n  \\[\n  f(n) = g(n) + h(n)\n  \\]\n  to choose which node to expand next. Nodes with lower f(n) are expanded first, steering the search toward the most promising reasoning paths.\n\n## How A* Works in Mathematical Reasoning\n\n1. **Initialization:**  \n   The algorithm starts with the initial problem as the root node, with an initial cost \\(g(n)=0\\).\n\n2. **Expansion:**  \n   From the current node, the model generates several potential next steps (child nodes). Each child node represents a possible continuation of the reasoning process.\n\n3. **Cost and Heuristic Calculation:**  \n   - **g(n):** Represents the cost accumulated so far (e.g., the number of reasoning steps taken).\n   - **h(n):** An estimate of how “far” the current state is from a complete final answer. This can be derived via LLM-based evaluations or comparisons to known correct patterns.\n\n4. **Priority Queue and Node Selection:**  \n   The algorithm uses a priority queue to maintain nodes, sorted by their \\( f(n) \\) value. The node with the smallest \\(f(n)\\) (i.e., the most promising combination of current cost and estimated remaining cost) is expanded next.\n\n5. **Goal Test:**  \n   The process continues until a node is found that meets the goal—a node whose state contains a complete final answer in the expected format (e.g., a LaTeX expression wrapped in `\\boxed{...}`).\n\n## Challenges in Applying A* to Reasoning\n\n- **Heuristic Design:**  \n  Defining an effective h(n) is challenging. The heuristic must correlate well with the true “distance” to a correct final answer. For language models, this might involve model confidence scores or custom prompt evaluations.\n\n- **Balancing Exploration and Exploitation:**  \n  Overemphasis on g(n) might favor shorter, less-complete reasoning chains, while too much reliance on h(n) might cause the search to overestimate the quality of partially correct answers.\n\n- **Computational Expense:**  \n  Evaluating each node’s heuristic (potentially via additional LLM queries) can be computationally expensive, especially in a large search space.\n\n- **Scalability:**  \n  The state space for reasoning is vast. A well-tuned A* algorithm can efficiently prune irrelevant paths, but without a robust heuristic, the number of nodes to explore can grow exponentially.\n\n\n\nNext, we will move on to the code implementation of the A* algorithm for our reasoning framework.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import re\nimport time\nimport heapq\nfrom typing import List, Optional\n\n\nclass AStarNode:\n    def __init__(self, state: str, g: float = 0.0, parent: Optional['AStarNode'] = None):\n        self.state = state                      # easy: store the current problem or partial answer\n        self.g = g                              # easy: cost so far (depth)\n        self.h: float = 0.0                     # will be set by heuristic evaluator\n        self.f: float = 0.0                     # f = g + h\n        self.parent = parent                    # link back to parent for solution path\n        self.children: List[AStarNode] = []     # easy: list to hold generated successors\n\n    def __lt__(self, other: 'AStarNode'):\n        return self.f < other.f                 # easy: allow heapq to compare nodes by f‑value\n\n    def __repr__(self):\n        preview = self.state if len(self.state) < 50 else self.state[:47] + \"...\"\n        return f\"AStarNode(g={self.g}, h={self.h}, f={self.f}, state='{preview}')\"\n\n\nclass AStarSearch:\n    def __init__(\n        self,\n        num_candidates: int = 3,\n        max_depth: int = 3,\n        verbose: bool = False,\n        max_fallback: int = 3\n    ):\n        self.num_candidates = num_candidates    # how many answers to generate per node\n        self.max_depth = max_depth              # search will stop at this depth\n        self.verbose = verbose                  # if True, print debug info\n        self.max_fallback = max_fallback        # retries for enforcing boxed format\n        self.root_problem: Optional[str] = None  # will hold the original problem text\n\n    def is_solution(self, state: str) -> bool:\n        # return True if `extract_answer(state)` yields a non‑empty string\n        return extract_answer(state) is not None\n\n    def generate_candidates(self, node: AStarNode) -> List[str]:\n        \"\"\"\n        Ask the LLM for `num_candidates` boxed answers to node.state.\n        \"\"\"\n        # compose a prompt with node.state asking for LaTeX \\\\boxed{...} answers\n        prompt = f\"\"\"You are solving a mathematics problem.\nGiven the following problem or partial solution:\n\n{node.state}\n\nGenerate {self.num_candidates} different possible final answers.\nEach answer should be clearly marked with a LaTeX \\\\boxed{{...}} command.\nIf it's a final answer, make sure it's enclosed in \\\\boxed{{...}}.\nWrite each answer on a new line. These answers should be distinct.\n\"\"\"\n        response = get_llm_response(prompt)\n        \n        # Split the response into non-empty lines\n        lines = [line.strip() for line in response.split('\\n') if line.strip()]\n        \n        # If fewer than num_candidates, replicate lines to reach that count\n        while len(lines) < self.num_candidates:\n            lines.append(lines[0] if lines else \"No solution found.\")\n            \n        # Return exactly num_candidates answer strings\n        return lines[:self.num_candidates]\n\n    def evaluate_heuristic(self, state: str) -> float:\n        \"\"\"\n        Use GPT (via get_api_response) to score how incomplete a candidate is:\n        0 means perfect boxed answer; higher means more incomplete.\n        \"\"\"\n        assert self.root_problem is not None, \"Root problem must be set before heuristic evaluation\"\n        \n        # build `eval_prompt` using self.root_problem and the current `state`\n        eval_prompt = f\"\"\"Given this original problem:\n{self.root_problem}\n\nAnd this current state of the solution:\n{state}\n\nOn a scale of 0 to 10:\n- Score 0 if this contains a complete, correctly formatted final answer inside \\\\boxed{{...}}\n- Score higher (up to 10) the more work remains to reach a final boxed answer\n\nReturn ONLY a numeric score without explanation.\n\"\"\"\n        response = get_api_response(eval_prompt).strip()\n        \n        # extract the first numeric value with `re.search`\n        match = re.search(r'(\\d+(?:\\.\\d+)?)', response)\n        \n        # return that float, or a fallback like `self.max_depth * 10` if parsing fails\n        return float(match.group(1)) if match else self.max_depth * 10\n\n    def expand_node(self, node: AStarNode) -> List[AStarNode]:\n        if self.verbose:\n            print(f\"\\nExpanding node at depth {node.g}:\\n{node.state}\\n\")\n            \n        # if self.root_problem is None, set it to node.state\n        if self.root_problem is None:\n            self.root_problem = node.state\n            \n        candidates = self.generate_candidates(node)\n        children: List[AStarNode] = []\n\n        for cand in candidates:\n            attempts = 0\n            while not self.is_solution(cand) and attempts < self.max_fallback:\n                fallback_prompt = f\"\"\"You are a mathematics expert. For this problem:\n{self.root_problem}\n\nUsing the partial solution:\n{cand}\n\nProvide ONLY the final answer enclosed in \\\\boxed{{...}} LaTeX format.\nDO NOT show any work, ONLY the final boxed answer.\n\"\"\"\n                response = get_llm_response(fallback_prompt)\n                cand = response.strip()\n                attempts += 1\n                if self.verbose:\n                    print(f\"Fallback attempt {attempts}: {'Solution found' if self.is_solution(cand) else 'Still no solution'}\")\n            \n            child = AStarNode(state=cand, g=node.g + 1, parent=node)\n            child.h = self.evaluate_heuristic(child.state)\n            child.f = child.g + child.h\n            node.children.append(child)\n            children.append(child)\n            \n            if self.verbose:\n                print(f\"Generated child: {child}\")\n        \n        return children\n\n    def search(self, initial_problem: str) -> Optional[AStarNode]:\n        \"\"\"\n        Run A* until a boxed solution is found or max_depth is exceeded.\n        \"\"\"\n        self.root_problem = initial_problem\n        root = AStarNode(state=initial_problem, g=0.0)\n        root.h = self.evaluate_heuristic(root.state)\n        root.f = root.g + root.h\n        frontier: List[AStarNode] = []\n        heapq.heappush(frontier, root)\n\n        while frontier:\n            node = heapq.heappop(frontier)\n            if self.verbose:\n                print(f\"Expanding: {node}\")\n            \n            if self.is_solution(node.state):\n                return node\n            \n            if node.g >= self.max_depth:\n                continue\n            \n            for child in self.expand_node(node):\n                heapq.heappush(frontier, child)\n\n        return None\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T15:16:31.038756Z",
     "iopub.execute_input": "2025-05-23T15:16:31.039063Z",
     "iopub.status.idle": "2025-05-23T15:16:31.054592Z",
     "shell.execute_reply.started": "2025-05-23T15:16:31.039043Z",
     "shell.execute_reply": "2025-05-23T15:16:31.053837Z"
    }
   },
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": "# Test Code for A* Search Method with Minimal Hyperparameters\n\nBelow is a description of the test procedure for the A* search method using minimal hyperparameters. This test ensures that the algorithm returns only the final answer in the proper format, without extra chain-of-thought text.\n\n- **Initialization:**\n  - An A* search instance is created with the following settings:\n    - `num_candidates`: 1 (only one candidate is generated per node)\n    - `max_depth`: 1 (the search tree is kept shallow for testing)\n    - `verbose`: True (detailed debug information is printed)\n    - `max_fallback`: 3 (up to three fallback attempts are made to force a final answer)\n  - The initial problem is set as:\n    - \"Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)\"\n\n- **Search Execution:**\n  - The A* search is executed on the initial problem to obtain a solution node.\n  \n- **Tree Printing:**\n  - A recursive function (e.g., `print_astar_tree`) is used to print the entire A* search tree, starting from the root. This allows inspection of all nodes and the reasoning process.\n\n- **Final Answer Extraction:**\n  - If a solution node is found, the algorithm backtracks to the root to print the entire tree.\n  - Then it extracts the final answer from the solution node using a helper function (e.g., `extract_answer`). The state of the solution node is reformatted to display only the final answer in the exact format (e.g., `\\boxed{<final answer>}`).\n\n- **Fallback Handling:**\n  - If no solution node is found, a fallback prompt is issued. This prompt instructs the model to provide ONLY its final answer in the correct format, with no extra explanation.\n  - The fallback final answer is then printed.\n\nThis test code is designed to verify that, with minimal hyperparameters, the A* search method consistently returns a complete final answer, ensuring the output is directly comparable with the expected result.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Test Code for A* Search Method with Minimal Hyperparameters\n\n# Initialize the A* search instance with minimal settings.\n# Candidate generation still uses get_llm_response(...),\n# but all verification and final fallback use get_api_response(...) (Gemini).\nastar = AStarSearch(\n    num_candidates=1,\n    max_depth=1,\n    verbose=True,\n    max_fallback=3\n)\n\ninitial_problem = \"Solve the integral: \\\\( \\\\int_0^1 x^2 \\\\, dx \\\\)\"\nsolution_node = astar.search(initial_problem)\n\n# Recursive printer for the A* search tree.\ndef print_astar_tree(node, indent=\"\"):\n    print(indent + repr(node))\n    for child in node.children:\n        print_astar_tree(child, indent + \"  \")\n\nif solution_node:\n    # Backtrack to the root.\n    root = solution_node\n    while root.parent is not None:\n        root = root.parent\n\n    # Print the entire tree from the root.\n    print_astar_tree(root)\n\n    # Extract and normalize the final answer.\n    final = extract_answer(solution_node.state)\n    if final:\n        solution_node.state = f\"\\\\boxed{{{final}}}\"\n    print(\"\\nFinal Answer Found:\")\n    print(solution_node.state)\nelse:\n    # No solution found: use GPT/Gemini directly for the final answer.\n    fallback_prompt = (\n        f\"Based on the problem: \\\"{initial_problem}\\\", please provide ONLY your final answer \"\n        \"in the exact format \\\\boxed{<final answer>}. Do not include any chain-of-thought or extra explanation.\"\n    )\n    forced_final_answer = get_api_response(fallback_prompt).strip()\n    final = extract_answer(forced_final_answer) or forced_final_answer\n    forced_final_answer = f\"\\\\boxed{{{final}}}\"\n    print(\"\\nNo complete solution node found. Forced final answer:\")\n    print(forced_final_answer)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T15:16:36.842033Z",
     "iopub.execute_input": "2025-05-23T15:16:36.842316Z",
     "iopub.status.idle": "2025-05-23T15:17:27.621288Z",
     "shell.execute_reply.started": "2025-05-23T15:16:36.842295Z",
     "shell.execute_reply": "2025-05-23T15:17:27.620479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Expanding: AStarNode(g=0.0, h=2.0, f=2.0, state='Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)')\n\nExpanding node at depth 0.0:\nSolve the integral: \\( \\int_0^1 x^2 \\, dx \\)\n\nFallback attempt 1: Solution found\nGenerated child: AStarNode(g=1.0, h=10.0, f=11.0, state='Okay, so I need to solve the integral \\( \\int_0...')\nExpanding: AStarNode(g=1.0, h=10.0, f=11.0, state='Okay, so I need to solve the integral \\( \\int_0...')\nAStarNode(g=0.0, h=2.0, f=2.0, state='Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)')\n  AStarNode(g=1.0, h=10.0, f=11.0, state='Okay, so I need to solve the integral \\( \\int_0...')\n\nFinal Answer Found:\n\\boxed{\\dfrac{1}{3}}\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": "# Evaluation of the A* Search Method on the Math500 Dataset\n\nThis evaluation framework is designed to test the A* search method on a subset of the Math500 dataset. It provides flexibility in hyperparameter configuration and is set up for both detailed output and large-scale evaluation. The framework saves results, summarizes key metrics, and includes mechanisms to force the model to output a final answer in the expected format.\n\n## Key Features\n\n- **Unique Results File:**  \n  Uses a dedicated results file (e.g., `evaluation_results_astar_test.json`) to store evaluation data. The file is cleared at the start of each run to prevent interference from previous evaluations.\n\n- **Configurable Hyperparameters:**  \n  You can adjust parameters such as:\n  - `num_candidates`: Number of candidate final answers generated per node.\n  - `max_depth`: Maximum depth of the search tree.\n  \n  These settings enable you to test different reasoning strategies and trade-offs between search exploration depth and the precision of the final answer.\n\n- **Sample Selection:**  \n  The `max_samples` parameter controls the number of problems from the dataset to evaluate. This allows you to start by testing on a single sample and then scale up to 100 or more problems as desired.\n\n- **Fallback Mechanisms:**  \n  If no node in the A* search tree contains a complete final answer (i.e., one wrapped in `\\boxed{...}`), a fallback prompt is issued to force the model to output only the final answer. This guarantees that every evaluated problem produces a final answer in the proper format.\n\n- **Final Answer Extraction:**  \n  After processing the search tree, the framework extracts just the final answer from the model’s output (using a helper function like `extract_answer`). This ensures that only the final answer is compared against the expected solution, without any additional chain-of-thought text.\n\n- **Result Saving and Analysis:**  \n  The framework saves detailed evaluation data—including the problem text, the model's raw response, the extracted final answer, and correctness—and produces a summary report with metrics such as total problems evaluated, number of correct answers, and overall accuracy.\n\n## Encouragement for Further Improvement\n\n**Prompt Engineering & Fallback Strategies:**  \nThe current method forces the model to provide a final answer using a fallback prompt if the initial search does not yield the required format. Experiment with:\n- **Prompt Tuning:** Adjust the wording and structure to further emphasize \"ONLY your final answer\" and \"no extra text.\"\n- **Iterative Refinement:** Consider iterative prompt refinement or additional post-processing to isolate the final answer more reliably.\n- **Innovative Approaches:** This challenge of extracting only the final answer is an active area of research. Exploring new strategies may lead to better performance and more robust results.\n\n**Scalability:**  \nOnce the hyperparameters and prompting strategy are fine-tuned on a small set of problems, scale the evaluation to a larger sample (e.g., 100+ problems). A broader evaluation can reveal trends and help identify further improvements.\n\n**Experiment and Innovate:**  \nFeel free to modify prompts, adjust hyperparameters, and refine fallback mechanisms. Comparing the A* search method with other approaches, such as the Tree of Thoughts method, can provide valuable insights. Your experimentation is key to developing a more robust and reliable evaluation system.\n\nThis flexible framework is designed to meet diverse research and production needs—keep iterating and exploring until you achieve optimal results!\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport random\nfrom tqdm import tqdm\n\ndef evaluate_astar_random(max_samples: int = 1):\n    \"\"\"\n    Evaluate the A* search method on randomly selected problems from the Math500 dataset.\n    Uses GPT/Gemini (via get_api_response) for any fallback final-answer requests.\n    Assumes existence of:\n      - load_math500_dataset()\n      - extract_answer(solution_text)\n      - compare_answers(correct_answer, predicted_answer)\n      - save_result(results_file, result_dict)\n      - load_existing_results(results_file)\n      - analyze_results(results_list)\n      - get_llm_response(prompt)   # for candidate generation\n      - get_api_response(prompt)   # for GPT/Gemini–based fallback\n    \"\"\"\n    os.makedirs(\"results\", exist_ok=True)\n    results_file = \"results/evaluation_results_astar_random_test.json\"\n    \n    # Clear previous test results\n    if os.path.exists(results_file):\n        os.remove(results_file)\n    \n    dataset = load_math500_dataset()\n    total = len(dataset)\n    \n    # Pick random unique indices\n    selected = set()\n    while len(selected) < max_samples:\n        selected.add(random.randint(0, total - 1))\n    \n    astar = AStarSearch(num_candidates=1, max_depth=1, verbose=True, max_fallback=3)\n    correct_count = 0\n    evaluated = 0\n    \n    def collect_all_nodes(node):\n        nodes = [node]\n        for c in node.children:\n            nodes.extend(collect_all_nodes(c))\n        return nodes\n    \n    for idx in selected:\n        item = dataset[idx]\n        problem_text = item['problem']\n        correct_answer = extract_answer(item['solution'])\n        \n        solution_node = astar.search(problem_text)\n        if solution_node:\n            response = solution_node.state\n            print(\"DEBUG: Found solution node:\", response)\n        else:\n            print(\"DEBUG: No solution node found. Using GPT fallback.\")\n            fallback_prompt = (\n                f\"Based on the problem: \\\"{problem_text}\\\", provide ONLY your final answer \"\n                \"in the exact format \\\\boxed{<final answer>}. Do not include any chain-of-thought or explanation.\"\n            )\n            response = get_api_response(fallback_prompt).strip()\n            print(\"DEBUG: Fallback response:\", response)\n        \n        predicted_answer = extract_answer(response)\n        if not predicted_answer:\n            print(\"DEBUG: predicted_answer is empty. Raw response was:\\n\", response)\n        \n        is_correct = compare_answers(correct_answer, predicted_answer or \"\")\n        \n        save_result(results_file, {\n            \"index\": idx,\n            \"problem\": problem_text,\n            \"response\": response,\n            \"correct_answer\": correct_answer,\n            \"predicted_answer\": predicted_answer,\n            \"is_correct\": is_correct\n        })\n        \n        if is_correct:\n            correct_count += 1\n        evaluated += 1\n        print(f\"Correct: {correct_count}/{evaluated} | Index: {idx}\")\n    \n    final_results = load_existing_results(results_file)\n    analyze_results(final_results)\n\n# Example usage:\nevaluate_astar_random(max_samples=30)\n# To test with different parameters, adjust max_samples.\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T15:17:51.149389Z",
     "iopub.execute_input": "2025-05-23T15:17:51.149660Z",
     "iopub.status.idle": "2025-05-23T16:01:13.894792Z",
     "shell.execute_reply.started": "2025-05-23T15:17:51.149640Z",
     "shell.execute_reply": "2025-05-23T16:01:13.894004Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "=== Results Summary ===\n",
    "\n",
    "Total problems: 30\n",
    "\n",
    "Correct answers: 18\n",
    "\n",
    "Accuracy: 60.00%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Monte Carlo Tree Search (MCTS) for Mathematical Reasoning\n\nMonte Carlo Tree Search (MCTS) is a probabilistic search algorithm particularly well-suited for decision-making tasks in large, complex search spaces. In the context of mathematical problem solving with language models, each node in the search tree represents a partial reasoning step. MCTS incrementally builds the tree by exploring the most promising reasoning paths through random simulations, balancing exploration of new paths with exploitation of those that appear promising.\n\n## Core Components of MCTS\n\nMCTS operates in four main stages:\n\n1. **Selection:**  \n   Starting at the root (the initial problem), the algorithm traverses the tree by selecting child nodes based on a policy that balances two factors:  \n   - **Exploitation:** Favoring nodes that have already shown high promise (i.e., those with a good reward or low cost).  \n   - **Exploration:** Giving a chance to less-visited nodes to discover potentially promising new paths.  \n   This balance is often managed by a criterion such as the Upper Confidence Bound (UCB).\n\n2. **Expansion:**  \n   When a leaf node is reached—one that has not been fully expanded—the algorithm expands it by generating one or more child nodes. Each new node represents a possible next step in the reasoning process, such as a candidate final answer generated by the language model.\n\n3. **Simulation (Rollout):**  \n   From the newly expanded node, the algorithm performs a simulation (or rollout) to estimate the outcome if that reasoning path were followed to completion. For mathematical reasoning, this may involve prompting the language model to complete the remaining reasoning and produce a final answer. The outcome of the simulation provides an estimated reward or cost for that path.\n\n4. **Backpropagation:**  \n   The result of the simulation is then propagated back up the tree. Each node along the path has its evaluation updated based on the outcome, which in turn refines the selection policy for future iterations. This backpropagation ensures that nodes contributing to more promising outcomes are prioritized.\n\n## How MCTS Applies to Mathematical Reasoning\n\n- **State Representation:**  \n  Each node encapsulates a partial solution or chain-of-thought produced by the language model. The goal is to eventually obtain a final answer formatted in a concise manner (e.g., wrapped in `\\boxed{...}`).\n\n- **Reward and Heuristic:**  \n  The reward signal derived from the simulation reflects how close a reasoning path is to a correct final answer. The model's evaluations—such as plausibility scores—help guide the search by penalizing incomplete or incorrect paths.\n\n- **Balancing Exploration and Exploitation:**  \n  MCTS effectively manages the trade-off between exploring new, untested reasoning paths and exploiting those paths that have already demonstrated potential. This balance is critical in navigating the vast space of possible reasoning steps.\n\n## Challenges and Considerations\n\n- **Simulation Cost:**  \n  Running multiple simulations per node can be computationally intensive, especially when each simulation involves multiple language model calls.\n\n- **Reward Signal Design:**  \n  Defining an accurate and meaningful reward (or evaluation) for partial solutions is challenging. The reward must correlate well with the likelihood of ultimately arriving at a complete and correct final answer.\n\n- **Parameter Tuning:**  \n  Effective implementation of MCTS requires careful tuning of parameters like the exploration constant and the number of simulations per node. This tuning is essential to balance the search effectively.\n\n- **Ensuring Concise Final Answers:**  \n  One of the key objectives is to force the model to output only the final answer (without additional chain-of-thought text). This requires precise prompt engineering and robust fallback strategies in both candidate generation and simulation phases.\n\n## Conclusion\n\nMCTS provides a powerful framework for exploring the reasoning process in language models by combining random simulation with informed backpropagation. Through iterative exploration of promising reasoning paths and careful balancing of exploration and exploitation, MCTS aims to identify the most promising route to a final answer. With a strong focus on ensuring that only a concise final answer is produced (wrapped in a format like `\\boxed{...}`), this method offers significant potential for enhancing mathematical problem solving.\n\nBy adjusting hyperparameters and refining the simulation and evaluation processes, you can experiment with different configurations to improve the efficiency and accuracy of the final answers. This makes MCTS a flexible and promising approach for further research and practical applications in guided reasoning with language models.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import re\nimport time\nimport math\nimport heapq\nfrom typing import List, Optional\n\n\ndef evaluate_with_gpt(problem: str, candidate: str) -> float:\n    \"\"\"\n    Ask the verifier LLM (via get_api_response) whether the boxed answer is correct.\n    Returns 1.0 for “yes”, 0.0 for “no”, or an intermediate score if provided.\n    \"\"\"\n    prompt = (\n        \"You are a precise math grader.\\n\\n\"\n        f\"Problem:\\n\\\"{problem}\\\"\\n\\n\"\n        \"Final answer to check (in \\\\boxed{...} format):\\n\"\n        f\"\\\"{candidate}\\\"\\n\\n\"\n        \"Is this answer correct? Reply with a number between 0 (incorrect) and 1 (fully correct).\"\n    )\n    # Send `prompt` to `get_api_response`, strip whitespace\n    response = get_api_response(prompt).strip()\n    # Extract the first numeric match with `re.search`\n    match = re.search(r'(\\d+(?:\\.\\d+)?)', response)\n    # Convert to float (fallback to 0.0 on failure)\n    score = float(match.group(1)) if match else 0.0\n    # Clamp the result to [0.0, 1.0] and return it\n    return max(0.0, min(1.0, score))\n\n\nclass MCTSNode:\n    def __init__(self, state: str, parent: Optional['MCTSNode'] = None):\n        self.state = state\n        self.parent = parent\n        self.children: List[MCTSNode] = []\n        self.visits = 0\n        self.total_reward = 0.0\n\n    def add_child(self, child: 'MCTSNode'):\n        self.children.append(child)\n\n    def __repr__(self):\n        preview = self.state if len(self.state) < 50 else self.state[:47] + \"...\"\n        return f\"MCTSNode(visits={self.visits}, reward={self.total_reward:.2f}, state='{preview}')\"\n\n\nclass MCTSSearch:\n    def __init__(\n        self,\n        num_simulations: int = 10,\n        exploration_const: float = 1.41,\n        max_depth: int = 3,\n        max_fallback: int = 3,\n        num_candidates: int = 1,\n        verbose: bool = False\n    ):\n        self.num_simulations = num_simulations\n        self.exploration_const = exploration_const\n        self.max_depth = max_depth\n        self.max_fallback = max_fallback\n        self.num_candidates = num_candidates\n        self.verbose = verbose\n        self.root_problem: Optional[str] = None\n\n    def is_solution(self, state: str) -> bool:\n        # return True if `extract_answer(state)` yields a non-empty boxed answer\n        return extract_answer(state) is not None\n\n    def generate_candidates(self, state: str) -> List[str]:\n        # build a prompt asking for `self.num_candidates` LaTeX \\\\boxed{...} answers to `state`\n        prompt = f\"\"\"You are solving a mathematics problem.\nGiven the following problem or partial solution:\n\n{state}\n\nGenerate {self.num_candidates} different possible final answers.\nEach answer should be clearly marked with a LaTeX \\\\boxed{{...}} command.\nIf it's a final answer, make sure it's enclosed in \\\\boxed{{...}}.\nWrite each answer on a new line. These answers should be distinct.\n\"\"\"\n        response = get_llm_response(prompt)\n\n        # split response into non-empty lines\n        lines = [line.strip() for line in response.split('\\n') if line.strip()]\n\n        # If fewer than num_candidates, replicate lines to reach that count\n        while len(lines) < self.num_candidates:\n            lines.append(lines[0] if lines else \"No solution found.\")\n\n        # Return exactly num_candidates answer strings\n        return lines[:self.num_candidates]\n\n    def expand(self, node: MCTSNode) -> List[MCTSNode]:\n        if self.verbose:\n            print(f\"\\nExpanding node (depth {self._depth(node)}):\\n{node.state}\\n\")\n\n        # ensure we remember the original problem\n        if self.root_problem is None:\n            self.root_problem = node.state\n\n        candidates = self.generate_candidates(node.state)\n        children: List[MCTSNode] = []\n\n        for cand in candidates:\n            final = cand\n            attempts = 0\n\n            while not self.is_solution(final) and attempts < self.max_fallback:\n                fallback_prompt = f\"\"\"Given this problem:\n{self.root_problem}\n\nAnd your current answer:\n{final}\n\nProvide ONLY the final answer enclosed in \\\\boxed{{...}} LaTeX format.\nDO NOT show any work, ONLY the final boxed answer.\n\"\"\"\n                response = get_llm_response(fallback_prompt)\n                final = response.strip()\n                attempts += 1\n                if self.verbose:\n                    print(f\"Fallback attempt {attempts}: {'Solution found' if self.is_solution(final) else 'Still no solution'}\")\n\n            child = MCTSNode(state=final, parent=node)\n            node.add_child(child)\n            children.append(child)\n\n        return children\n\n    def simulate(self, node: MCTSNode) -> float:\n        \"\"\"\n        Instead of a blind rollout, directly evaluate the final boxed answer.\n        \"\"\"\n        if self.root_problem is None:\n            return 0.0\n\n        if not self.is_solution(node.state):\n            fallback_prompt = f\"\"\"Given this problem:\n{self.root_problem}\n\nProvide ONLY the final answer enclosed in \\\\boxed{{...}} LaTeX format.\nDO NOT show any work, ONLY the final boxed answer.\n\"\"\"\n            response = get_llm_response(fallback_prompt)\n            node.state = response.strip()\n\n        return evaluate_with_gpt(self.root_problem, node.state)\n\n    def ucb_score(self, child: MCTSNode, parent_visits: int) -> float:\n        if child.visits == 0:\n            return float('inf')\n\n        exploit = child.total_reward / child.visits\n        explore = self.exploration_const * math.sqrt(math.log(parent_visits) / child.visits)\n        return exploit + explore\n\n    def select(self, root: MCTSNode) -> MCTSNode:\n        # starting at `root`, repeatedly pick the child with highest `ucb_score`\n        # until you reach a node with no children, then return it\n        node = root\n        while node.children:\n            best_score = float('-inf')\n            best_child = None\n            \n            for child in node.children:\n                score = self.ucb_score(child, node.visits)\n                if score > best_score:\n                    best_score = score\n                    best_child = child\n                    \n            if best_child is None:\n                break\n            node = best_child\n            \n        return node\n\n    def backpropagate(self, node: MCTSNode, reward: float):\n        while node is not None:\n            node.visits += 1\n            node.total_reward += reward\n            node = node.parent\n\n    def _depth(self, node: MCTSNode) -> int:\n        d = 0\n        while node.parent:\n            d += 1\n            node = node.parent\n        return d\n\n    def search(self, initial_problem: str) -> Optional[MCTSNode]:\n        \"\"\"\n        Perform MCTS using GPT as the verifier:\n        - Selection: `select`\n        - Expansion: `expand`\n        - Simulation/Backprop: `simulate` + `backpropagate`\n        \"\"\"\n        self.root_problem = initial_problem\n        root = MCTSNode(state=initial_problem)\n        \n        for _ in range(self.num_simulations):\n            leaf = self.select(root)\n            \n            if self._depth(leaf) >= self.max_depth:\n                reward = self.simulate(leaf)\n                self.backpropagate(leaf, reward)\n                continue\n                \n            children = self.expand(leaf)\n            if children:\n                reward = self.simulate(children[0])\n                self.backpropagate(children[0], reward)\n                \n        if root.children:\n            best_child = max(root.children, key=lambda c: c.total_reward / max(c.visits, 1))\n            return best_child\n            \n        return root\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T16:03:24.519148Z",
     "iopub.execute_input": "2025-05-23T16:03:24.519408Z",
     "iopub.status.idle": "2025-05-23T16:03:24.538441Z",
     "shell.execute_reply.started": "2025-05-23T16:03:24.519391Z",
     "shell.execute_reply": "2025-05-23T16:03:24.537605Z"
    }
   },
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": "# Test Code for MCTS Search Method with Minimal Hyperparameters\n\nBelow is a description of the test procedure for the MCTS search method using minimal hyperparameters. This test ensures that the algorithm returns only the final answer in the proper format, without extra chain-of-thought text.\n\n- **Initialization:**\n  - An MCTS search instance is created with the following settings:\n    - `num_simulations`: 5 (number of simulations to run for exploring reasoning paths)\n    - `exploration_const`: 1.41 (parameter to balance exploration and exploitation)\n    - `max_depth`: 1 (the search tree is kept shallow for testing)\n    - `max_fallback`: 3 (up to three fallback attempts are made to force a final answer)\n    - `num_candidates`: 1 (only one candidate is generated per node)\n    - `verbose`: True (detailed debug information is printed)\n  - The initial problem is set as:\n    - \"Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)\"\n\n- **Search Execution:**\n  - The MCTS search is executed on the initial problem to obtain a solution node.\n\n- **Tree Printing:**\n  - A recursive function (e.g., `print_mcts_tree`) prints the entire MCTS search tree starting from the root. This allows inspection of all nodes and the reasoning process.\n\n- **Final Answer Extraction:**\n  - If a solution node is found, the algorithm backtracks to the root and prints the full tree.\n  - The final answer is then extracted from the solution node using a helper function (e.g., `extract_answer`) and reformatted to display only the final answer in the exact format (e.g., `\\boxed{<final answer>}`), ensuring that no extra explanation or chain-of-thought text is present.\n\n- **Fallback Handling:**\n  - If no solution node is found, a fallback prompt is issued that instructs the model to provide ONLY the final answer in the correct format, with no additional explanation.\n  - The fallback final answer is printed.\n\nThis test code is designed to verify that, with minimal hyperparameters, the MCTS search method consistently returns a complete final answer. Users are encouraged to experiment with these hyperparameters and refine the prompts to further improve performance and compare the results with other methods.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Test Code for MCTS Search Method with Minimal Hyperparameters\n\n# Initialize the MCTS search instance with minimal settings.\n# Generation uses get_llm_response(...), evaluation/fallback uses get_api_response(...)\nmcts = MCTSSearch(\n    num_simulations=5,\n    exploration_const=1.41,\n    max_depth=1,\n    max_fallback=3,\n    num_candidates=1,\n    verbose=True\n)\n\ninitial_problem = \"Solve the integral: \\\\( \\\\int_0^1 x^2 \\\\, dx \\\\)\"\nsolution_node = mcts.search(initial_problem)\n\n# Recursive printer for the MCTS tree.\ndef print_mcts_tree(node, indent=\"\"):\n    print(indent + repr(node))\n    for child in node.children:\n        print_mcts_tree(child, indent + \"  \")\n\nif solution_node:\n    # Backtrack to the root node.\n    root = solution_node\n    while root.parent is not None:\n        root = root.parent\n\n    # Print the entire tree from the root.\n    print_mcts_tree(root)\n\n    # Extract and normalize the final boxed answer.\n    final = extract_answer(solution_node.state)\n    if final:\n        solution_node.state = f\"\\\\boxed{{{final}}}\"\n    print(\"\\nFinal Answer Found:\")\n    print(solution_node.state)\nelse:\n    # No solution found: use GPT/Gemini verifier directly for final answer.\n    fallback_prompt = (\n        f\"Based on the problem: \\\"{initial_problem}\\\", please provide ONLY your final answer \"\n        \"in the exact format \\\\boxed{<final answer>}. Do not include any chain-of-thought or extra explanation.\"\n    )\n    forced_final_answer = get_api_response(fallback_prompt).strip()\n    final = extract_answer(forced_final_answer) or forced_final_answer\n    forced_final_answer = f\"\\\\boxed{{{final}}}\"\n    print(\"\\nNo complete solution found. Forced final answer:\")\n    print(forced_final_answer)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T16:03:29.271056Z",
     "iopub.execute_input": "2025-05-23T16:03:29.271347Z",
     "iopub.status.idle": "2025-05-23T16:04:14.716847Z",
     "shell.execute_reply.started": "2025-05-23T16:03:29.271330Z",
     "shell.execute_reply": "2025-05-23T16:04:14.716194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\nExpanding node (depth 0):\nSolve the integral: \\( \\int_0^1 x^2 \\, dx \\)\n\nFallback attempt 1: Solution found\nMCTSNode(visits=5, reward=5.00, state='Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)')\n  MCTSNode(visits=5, reward=5.00, state='Okay, so I have to solve the integral \\( \\int_0...')\n\nFinal Answer Found:\n\\boxed{\\dfrac{1}{3}}\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": "# Evaluation of MCTS Search Method on the Math500 Dataset\n\nThis evaluation framework is designed to test the MCTS search method on a subset of the Math500 dataset. The framework is highly configurable via hyperparameters and forces the model to output only the final answer in the expected format (e.g., `\\boxed{<final answer>}`) with no additional chain-of-thought text.\n\n## Key Features\n\n- **Unique Results File:**  \n  A dedicated results file (e.g., `evaluation_results_mcts_test.json`) is used to save evaluation data. The file is cleared at the beginning of each run to ensure a fresh start without interference from previous evaluations.\n\n- **Configurable Hyperparameters:**  \n  You can adjust parameters such as:\n  - `num_simulations`: Number of MCTS simulations used to explore reasoning paths.\n  - `exploration_const`: The constant used in the UCB formula to balance exploration and exploitation.\n  - `max_depth`: Maximum depth of the search tree.\n  - `max_fallback`: Maximum number of fallback attempts to force the model to output a final answer.\n  - `num_candidates`: Number of candidate final answers generated per node.\n  \n  These settings allow you to experiment with different reasoning strategies and trade-offs between search depth and the precision of the final answer.\n\n- **Sample Selection:**  \n  The `max_samples` parameter controls the number of problems from the dataset to evaluate. This lets you start by testing on a single sample and then scale the evaluation to larger subsets (e.g., 100 or more problems) as desired.\n\n- **Final Answer Extraction and Fallback Mechanism:**  \n  The evaluation process extracts only the final answer from the model’s output (using a helper like `extract_answer`), ensuring that only a concise final answer is compared against the expected solution. If no complete final answer is found within the search tree, a fallback prompt is issued to force the model to provide the final answer in the correct format.\n\n- **Result Saving and Analysis:**  \n  Each problem’s evaluation result—including the problem, the model’s raw response, the extracted final answer, and correctness—is saved to the results file. A summary report is then generated, which includes key metrics such as total evaluated problems, number of correct answers, and overall accuracy.\n\n## Encouragement for Further Improvement\n\n- **Experiment with the Exploration Constant:**  \n  Try using different values for the exploration constant (e.g., 0.5, 1.41, 2.0) to see how they affect the balance between exploring new nodes and exploiting known promising ones. Compare the results and observe how the search tree structure and the final answer accuracy change with each setting.\n\n- **Tuning Other Parameters:**  \n  Experiment with other hyperparameters such as `num_simulations`, `max_depth`, and `num_candidates`. Adjusting these values can impact the thoroughness of the search and the likelihood of obtaining a complete final answer. \n\n- **Document Your Observations:**  \n  As you tweak the parameters, please comment on your expectations and the outcomes:\n  - What changes do you observe when you adjust the exploration constant?\n  - How does increasing the maximum depth influence the quality and correctness of the final answer?\n  - Does generating more candidates per node lead to more accurate results?\n  \n  Sharing your observations and comparing them with results from other approaches (such as A* or the Tree of Thoughts method) can provide valuable insights and help guide further improvements.\n\nThis framework is designed to be flexible, so feel free to adjust the parameters and prompts to suit your research needs and push the performance of the MCTS method further.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nfrom tqdm import tqdm\n\ndef evaluate_mcts(max_samples=1):\n    \"\"\"\n    Evaluate the MCTS search method on a subset of Math500, using GPT/Gemini\n    (via get_api_response) for any fallback final‑answer requests.\n    Assumes these helpers exist:\n      - load_math500_dataset()\n      - extract_answer(text)\n      - compare_answers(correct, pred)\n      - save_result(filename, result_dict)\n      - load_existing_results(filename)\n      - analyze_results(results_list)\n      - get_llm_response(prompt)   # for candidate generation\n      - get_api_response(prompt)   # for GPT/Gemini verification/fallback\n    \"\"\"\n    os.makedirs(\"results\", exist_ok=True)\n    results_file = \"results/evaluation_results_mcts_test.json\"\n    if os.path.exists(results_file):\n        os.remove(results_file)\n\n    dataset = load_math500_dataset()\n    existing = load_existing_results(results_file)\n    processed = {r['index'] for r in existing}\n\n    mcts = MCTSSearch(\n        num_simulations=5,\n        exploration_const=1.41,\n        max_depth=1,\n        max_fallback=3,\n        num_candidates=1,\n        verbose=True\n    )\n\n    correct_count = 0\n    evaluated = 0\n\n    def collect_all_nodes(node):\n        nodes = [node]\n        for child in node.children:\n            nodes.extend(collect_all_nodes(child))\n        return nodes\n\n    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n        if idx in processed or evaluated >= max_samples:\n            continue\n\n        problem_text = item['problem']\n        correct_answer = extract_answer(item['solution'])\n\n        # Run MCTS search\n        solution_node = mcts.search(problem_text)\n\n        if solution_node:\n            response = solution_node.state\n            print(\"DEBUG: Found solution node:\", response)\n        else:\n            print(\"DEBUG: No solution node found. Using GPT fallback.\")\n            fallback_prompt = (\n                f\"Based on the problem: \\\"{problem_text}\\\", provide ONLY your final answer \"\n                \"in the exact format \\\\boxed{<final answer>}. Do not include any chain‑of‑thought or explanation.\"\n            )\n            response = get_api_response(fallback_prompt).strip()\n            print(\"DEBUG: Fallback response:\", response)\n\n        predicted_answer = extract_answer(response) or \"\"\n        if not predicted_answer:\n            print(\"DEBUG: predicted_answer is empty. Raw response was:\\n\", response)\n\n        is_correct = compare_answers(correct_answer, predicted_answer)\n        save_result(results_file, {\n            \"index\": idx,\n            \"problem\": problem_text,\n            \"response\": response,\n            \"correct_answer\": correct_answer,\n            \"predicted_answer\": predicted_answer,\n            \"is_correct\": is_correct\n        })\n\n        if is_correct:\n            correct_count += 1\n        evaluated += 1\n        print(f\"Correct: {correct_count}/{evaluated} | Index: {idx}\")\n\n    final_results = load_existing_results(results_file)\n    analyze_results(final_results)\n\n# Example: evaluate a single sample\nevaluate_mcts(max_samples=30)\n# To test more problems, increase max_samples\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-23T16:05:14.167028Z",
     "iopub.execute_input": "2025-05-23T16:05:14.167431Z",
     "iopub.status.idle": "2025-05-23T17:09:30.206916Z",
     "shell.execute_reply.started": "2025-05-23T16:05:14.167408Z",
     "shell.execute_reply": "2025-05-23T17:09:30.206289Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "=== Results Summary ===\n",
    "\n",
    "Total problems: 30\n",
    "\n",
    "Correct answers: 18\n",
    "\n",
    "Accuracy: 60.00%\n",
    "\n",
    "Runtime: 1:04:14\n"
   ]
  }
 ]
}
