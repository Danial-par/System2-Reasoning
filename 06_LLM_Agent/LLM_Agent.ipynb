{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCcHMgj69L26"
   },
   "source": [
    "<br>\n",
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<font color=0F5298 size=7>\n",
    "    System 2 - Homework 3<br>\n",
    "<font color=2565AE size=5>\n",
    "    Spring 2025<br>\n",
    "<font color=3C99D size=5>\n",
    "    LLM Agent <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awptGtyv9L27"
   },
   "source": [
    "### Full Name: Danial Parnian\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRRAx_d89L27"
   },
   "source": [
    "# 🤖📘 Welcome to the *LLM + Agents* Learning Notebook!\n",
    "\n",
    "This interactive notebook is designed to help you **build intelligent agents** that leverage the power of **Large Language Models (LLMs)** and **Vision-Language Models (VLMs)** to reason about visual data and answer questions.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 What You'll Learn\n",
    "\n",
    "✅ Load and explore a visual question answering dataset  \n",
    "✅ Work with a powerful VLM (`QwenVLM`)  \n",
    "✅ Build a **judge model** to assess answer correctness  \n",
    "✅ Perform **zero-shot evaluation** of an LLM-based agent  \n",
    "✅ ✨ **Design and implement your own agent** that can reason step-by-step  \n",
    "\n",
    "---\n",
    "\n",
    "You'll incrementally complete missing components in code cells, guided by clear instructions. Use your creativity to make your agent smarter!\n",
    "\n",
    "> 💡 *Tip: Don’t hesitate to explore how prompting affects the agent’s behavior.*\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Setup\n",
    "\n",
    "Make sure you have the right environment, dependencies installed, and access to the required models.\n",
    "\n",
    "Now, let’s start building!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqcIDxLZiXgW"
   },
   "source": [
    "# HW3 - Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wd_Xr67Zk0P6"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bm0ZbhJk0Bq",
    "outputId": "432d6997-ebd9-4160-f9af-7b15fc92b757"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Archive:  agent_data.zip\n",
      "   creating: agent_data/\n",
      "  inflating: agent_data/data.csv     \n",
      "   creating: agent_data/images/\n",
      "  inflating: agent_data/images/1018.png  \n",
      "  inflating: agent_data/images/10461.png  \n",
      "  inflating: agent_data/images/10546.png  \n",
      "  inflating: agent_data/images/10916.png  \n",
      "  inflating: agent_data/images/11286.png  \n",
      "  inflating: agent_data/images/11507.png  \n",
      "  inflating: agent_data/images/12013.png  \n",
      "  inflating: agent_data/images/12120.png  \n",
      "  inflating: agent_data/images/12394.png  \n",
      "  inflating: agent_data/images/12454.png  \n",
      "  inflating: agent_data/images/12758.png  \n",
      "  inflating: agent_data/images/1289.png  \n",
      "  inflating: agent_data/images/12902.png  \n",
      "  inflating: agent_data/images/13049.png  \n",
      "  inflating: agent_data/images/1307.png  \n",
      "  inflating: agent_data/images/13083.png  \n",
      "  inflating: agent_data/images/13489.png  \n",
      "  inflating: agent_data/images/13609.png  \n",
      "  inflating: agent_data/images/1365.png  \n",
      "  inflating: agent_data/images/13685.png  \n",
      "  inflating: agent_data/images/13711.png  \n",
      "  inflating: agent_data/images/14126.png  \n",
      "  inflating: agent_data/images/14275.png  \n",
      "  inflating: agent_data/images/14331.png  \n",
      "  inflating: agent_data/images/14341.png  \n",
      "  inflating: agent_data/images/14859.png  \n",
      "  inflating: agent_data/images/14958.png  \n",
      "  inflating: agent_data/images/1503.png  \n",
      "  inflating: agent_data/images/15475.png  \n",
      "  inflating: agent_data/images/15538.png  \n",
      "  inflating: agent_data/images/15592.png  \n",
      "  inflating: agent_data/images/16494.png  \n",
      "  inflating: agent_data/images/16589.png  \n",
      "  inflating: agent_data/images/1718.png  \n",
      "  inflating: agent_data/images/17371.png  \n",
      "  inflating: agent_data/images/17388.png  \n",
      "  inflating: agent_data/images/17404.png  \n",
      "  inflating: agent_data/images/17523.png  \n",
      "  inflating: agent_data/images/17993.png  \n",
      "  inflating: agent_data/images/18117.png  \n",
      "  inflating: agent_data/images/18196.png  \n",
      "  inflating: agent_data/images/18342.png  \n",
      "  inflating: agent_data/images/18399.png  \n",
      "  inflating: agent_data/images/18412.png  \n",
      "  inflating: agent_data/images/18493.png  \n",
      "  inflating: agent_data/images/18532.png  \n",
      "  inflating: agent_data/images/18533.png  \n",
      "  inflating: agent_data/images/18796.png  \n",
      "  inflating: agent_data/images/18885.png  \n",
      "  inflating: agent_data/images/19101.png  \n",
      "  inflating: agent_data/images/1914.png  \n",
      "  inflating: agent_data/images/19140.png  \n",
      "  inflating: agent_data/images/19161.png  \n",
      "  inflating: agent_data/images/19319.png  \n",
      "  inflating: agent_data/images/19450.png  \n",
      "  inflating: agent_data/images/19494.png  \n",
      "  inflating: agent_data/images/19534.png  \n",
      "  inflating: agent_data/images/19570.png  \n",
      "  inflating: agent_data/images/19748.png  \n",
      "  inflating: agent_data/images/1981.png  \n",
      "  inflating: agent_data/images/1983.png  \n",
      "  inflating: agent_data/images/19959.png  \n",
      "  inflating: agent_data/images/2085.png  \n",
      "  inflating: agent_data/images/2132.png  \n",
      "  inflating: agent_data/images/2293.png  \n",
      "  inflating: agent_data/images/23.png  \n",
      "  inflating: agent_data/images/2342.png  \n",
      "  inflating: agent_data/images/2950.png  \n",
      "  inflating: agent_data/images/3056.png  \n",
      "  inflating: agent_data/images/3152.png  \n",
      "  inflating: agent_data/images/320.png  \n",
      "  inflating: agent_data/images/324.png  \n",
      "  inflating: agent_data/images/3901.png  \n",
      "  inflating: agent_data/images/4638.png  \n",
      "  inflating: agent_data/images/4865.png  \n",
      "  inflating: agent_data/images/5192.png  \n",
      "  inflating: agent_data/images/523.png  \n",
      "  inflating: agent_data/images/5628.png  \n",
      "  inflating: agent_data/images/5798.png  \n",
      "  inflating: agent_data/images/5950.png  \n",
      "  inflating: agent_data/images/6299.png  \n",
      "  inflating: agent_data/images/677.png  \n",
      "  inflating: agent_data/images/6826.png  \n",
      "  inflating: agent_data/images/6882.png  \n",
      "  inflating: agent_data/images/6897.png  \n",
      "  inflating: agent_data/images/6971.png  \n",
      "  inflating: agent_data/images/7041.png  \n",
      "  inflating: agent_data/images/7296.png  \n",
      "  inflating: agent_data/images/8029.png  \n",
      "  inflating: agent_data/images/8079.png  \n",
      "  inflating: agent_data/images/8210.png  \n",
      "  inflating: agent_data/images/8213.png  \n",
      "  inflating: agent_data/images/8663.png  \n",
      "  inflating: agent_data/images/8785.png  \n",
      "  inflating: agent_data/images/8891.png  \n",
      "  inflating: agent_data/images/8953.png  \n",
      "  inflating: agent_data/images/9118.png  \n",
      "  inflating: agent_data/images/9325.png  \n",
      "  inflating: agent_data/images/9340.png  \n",
      "  inflating: agent_data/images/9588.png  \n"
     ]
    }
   ],
   "source": [
    "!unzip agent_data.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-vT9w-09L28"
   },
   "source": [
    "```markdown\n",
    "**Guideline:** The following code cell that loads the dataset has been removed. Please write code to:\n",
    "1. Load the CSV file `agent_data/data.csv` into a pandas DataFrame.\n",
    "2. Display the first few rows to verify that the data has been loaded correctly.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59yrVwnJl9FZ",
    "outputId": "8eb3941f-d15c-4710-c54f-5df59c200770"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Image                                           question  answer\n",
      "0   2085  What is the shape of the object that is furthe...  square\n",
      "1  14958  Is the yellow object on the top or on the bottom?  bottom\n",
      "2  11507     Is the red object on the left or on the right?   right\n",
      "3   5798  Is the yellow object on the left or on the right?   right\n",
      "4   1289              What is the shape of the blue object?  circle\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('agent_data/data.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KyphWq3ImeMR"
   },
   "outputs": [],
   "source": [
    "x = data['question'].to_list()\n",
    "y = data['answer'].to_list()\n",
    "image_list = data['Image'].to_list()\n",
    "\n",
    "image_list = ['agent_data/images/'+str(i)+'.png' for i in image_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d74Tl1LM9L29"
   },
   "source": [
    "```markdown\n",
    "**Guideline:** Please:\n",
    "1. Implement a function named `show_data(i, x, y, image_list)` that:\n",
    "   - Prints the question `x[i]` and its corresponding answer `y[i]`.\n",
    "   - Displays the image at `image_list[i]` using `matplotlib`.\n",
    "2. Call this function for index `0` to visualize a sample from the dataset.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "sPsTqu-erB-Z",
    "outputId": "b1024183-dfb8-45be-e26a-44c40c28ee43"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "What is the shape of the object that is furthest from the gray object?\n",
      "square\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def show_data(i, x, y, image_list):\n",
    "    # Display the question and answer\n",
    "    print(x[i])\n",
    "    print(y[i])\n",
    "\n",
    "    # Load and display the image\n",
    "    img = Image.open(image_list[i])\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the first sample\n",
    "show_data(0, x, y, image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAITl_AdlU-O"
   },
   "source": [
    "## Load LVLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTo0X2GszL8H",
    "outputId": "b84bae73-4838-420c-ef41-05d3ca67b4fa"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting qwen-vl-utils==0.0.8 (from qwen-vl-utils[decord]==0.0.8)\n",
      "  Downloading qwen_vl_utils-0.0.8-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting av (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8)\n",
      "  Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (24.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (11.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2.32.3)\n",
      "Collecting decord (from qwen-vl-utils[decord]==0.0.8)\n",
      "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from decord->qwen-vl-utils[decord]==0.0.8) (2.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2025.4.26)\n",
      "Downloading qwen_vl_utils-0.0.8-py3-none-any.whl (5.9 kB)\n",
      "Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m35.3/35.3 MB\u001B[0m \u001B[31m21.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.6/13.6 MB\u001B[0m \u001B[31m88.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: decord, av, qwen-vl-utils\n",
      "Successfully installed av-14.4.0 decord-0.6.0 qwen-vl-utils-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install qwen-vl-utils[decord]==0.0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNZGzrDH9L29"
   },
   "source": [
    "```markdown\n",
    "**Guideline:** In the following cell, you will see how to load a pretrained Vision-Language Model (VLM) using the `transformers` library and related utilities.\n",
    "- Carefully read the code to understand:\n",
    "  1. How the model and processor are initialized.\n",
    "  2. How input images and prompts are preprocessed.\n",
    "  3. How `model.generate` is used to produce text outputs.\n",
    "- Be prepared to explain:\n",
    "  - The role of `AutoProcessor` and `Qwen2_5_VLForConditionalGeneration`.\n",
    "  - Why we trim generated token IDs before decoding.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592,
     "referenced_widgets": [
      "26cb7b2101d648ad91b044311dfcfbd0",
      "a54010eb662f49c788d288fa2bffbb69",
      "44f97e81dfe940efbc4d094458526ac6",
      "a3de7601ce9e495e853acded9948ab2c",
      "9dbc3c7b15ec47c6b0e889de5a159311",
      "0cc302c4fb3f4a29b765a42c699a6c0d",
      "7589b7cad78449aab09cc1a0855347a5",
      "525d6e9baf5b41fab8aab5450ac2f04b",
      "13698d4536514eaab9385bc54fe4ee80",
      "0e420edd9e95473e9bf6e672df31acb8",
      "26d1024349ec439ea93695ea4e3ed510",
      "12d980f73ef848e7bbffa96fac540b78",
      "378507fa37d3445ba6bebb1f7b06abea",
      "a3a5fa3a834d4fc89c2a42fc70c12c51",
      "8ade136ed3944a91932c4b99f16795c7",
      "09ad75fe8a7c4856937b70a222d0e0d3",
      "e2dbf74506584b6db0067319fdb7dbe5",
      "491604f93852445c857067846eea118f",
      "d8ae6c422f3d455698d5080aca55ede2",
      "01dfccd2f8d746559a96b4ab6aa948ce",
      "9b424185336c4d40923f02d530889208",
      "7ba9acfa4d494047bb8b7470888ecf9b",
      "f85f44c2f8bc4a7c948b803d59cc8ffe",
      "242b6212136049fba9f6ace4e6d57b2b",
      "8d38cad0c2cc4ef4bdedf9d241c6b71c",
      "9b781c7f7fe74630845e9cd47d9249a8",
      "9305c360fc014d659a812b89510dc44c",
      "8fce128b5cda483f901c2813c93ed9dc",
      "e61f89195c704464ae82fdf6bb36b728",
      "de4b305160f0410c9afc89d71423dbaf",
      "977066696b41445cb191b08465665287",
      "8fa71fd478024be28cac0f3d56f152de",
      "52864bb2875a4421890b898bd403da61",
      "c33eb10fada64553bd95487f8688165e",
      "6c1982ef4df749c9a217675e6075a9f0",
      "f191977868c048a191e9bd5504adf087",
      "3f8af5fee0b64412830d81fb9ba1a6f8",
      "09414e142d2447f9b8ab3afd9483016e",
      "d9e5ebc245084020ad8ed3ece4c95007",
      "e735f9e493cb435eb0f51beab0092737",
      "3a1bd70c02f04760b37a559b50ce6cea",
      "13afde3bc87c4e5889e409fe4fbacadb",
      "2f6cd31ea8a045bca579a400da9fae1d",
      "4b6db593efc14e4b9783d205c4ca455c",
      "e1c6e740d8184e4a869a9bd16015bd9f",
      "cbddacea0b364417b4cb45420413ecc0",
      "a43f455c4f2f4f5595de65e9931a468c",
      "eed31c6c7d61468a9e7d1059ae113616",
      "24a21a7c0e434cfd805ce42d2c0d1708",
      "8780512abab145f7bd66b65dce89822b",
      "20b8b615966c43b2a228cdaf2d4221bd",
      "71e792ff61314ff9b41a16014d90f5a1",
      "c848b8c7f98448c6bd737b5a36eb5875",
      "f76ed6d99d134d4eaf8903c7ad789ddc",
      "4a41cbabaa3b4dc38644456093d9d60a",
      "27360250220948dfa8d0b819fa9a90f9",
      "ce28f6224d9240de914019bc286cfe13",
      "601c4b4c5e2f4a1cb6b4b329a89b0d77",
      "dba6deccbc22415a81290752d72e3820",
      "edda1d56156243ba99bf27f812513350",
      "42fe2ae39b0e487fb5798df19f03079d",
      "d610eb18bd4840d4a4345fe8917e021e",
      "6cf0520ebcfb484cbfd0b3924c6876e9",
      "269b6f778f3447f5b95655ecc16592b7",
      "c60ba08baec04c898662fe8557a0d45c",
      "fa308ae69b994a32a0b5312d2744abd9",
      "11846300960d4ef797fc2a28aa00dddb",
      "7bbdefcb105342bf917283dd15032567",
      "6ad7dfdd234448eaa62fb3c1e66b40d0",
      "a86671bc81d34fd1a83428855f6b892b",
      "4675406343704258a7278ed3fa7a9d4b",
      "d42652cea02b4496b5a2de887a2fdaf1",
      "3cf0f0d640f14e34aff761e7b773efd4",
      "12060ffbcf5149bd9274e85bd403d8ac",
      "b55f0907c7524435ba70a900a82d3d8a",
      "e695be6939814088a5ebea992de0d170",
      "74f747de63f74ea29b11d7239e390bfa",
      "8c996b4a8ff245749f7d98f01bc4516f",
      "8126413acbb44a19a6cb5a0f706d3c6e",
      "fd981f7fca044904aecb6ee0a940be04",
      "8541e0fa2dba4e03a8e65a7991fae97d",
      "ecbdd0b09a2c4deaba65fad2579f49fa",
      "b19d597e08924a32918696c49f3f8b72",
      "c652be2f7902446c87f7533d7dac26de",
      "a67d34b399d449ca9faf845a84f8905b",
      "738e565624af461b85affe3026e819d3",
      "12cf3be14a2a439082dcaa2b89d597ac",
      "da04e2f1fcaa4552a1d60b3b247849ff",
      "3f1e8acdf17d41a7b24f7f2ec84f9044",
      "e2e606a0bb2741c583b19045a8f0bf64",
      "bc371df8d7a14df69382027e2abc091f",
      "82927da1490f49ac8057946ad386eb05",
      "bb181be1509941dd8cba6f0720b9e32e",
      "ceec088f4717410eb26c748600691fb1",
      "c2f6dede714345329ff909e581b11c84",
      "f8a6953a7e9340c6be12d206fecc7607",
      "8f55bd781ac14927a09a86f194fdcc71",
      "82cc87f9b3264153b0e94c04a61943d3",
      "db154813f0884366ad366a7c6b200305",
      "7ef0ffdb03334aa29f980975e7dd91ea",
      "56ddab770cb045c08d5374d0772cacf2",
      "d942248940084c32b78532b33116a195",
      "e564370ff7a04b349cd84df66236e804",
      "4b960708fbb44c6cac350f9621d4e7c3",
      "34836b1aaa85411fa949b351347ce490",
      "b7638385614948ceb9350bf0458ffe86",
      "f544b9c341fe40c8b206b041efca428a",
      "accc690b0597491297bc4fe2390f05e4",
      "253b9bdcf9b24a4d8a86f16b85adf657",
      "70750058beee4d65b21ccb5bcd067f43",
      "0c55827dfff748b2ab1866abafc7e8ef",
      "354d70e4eaeb439a9934bbed513ccdef",
      "e4c3e9561f8b4c43be6f9a5d7dd760c5",
      "49433a01a5ef45bca5a8fe628d55ef13",
      "000e1f763c0b40a688137a88044e67d8",
      "fb5b28e53c674f32bd2738d62d976b1c",
      "2d0b73f017904aada5343ef8f92d650e",
      "734719984c4947b8a3a968b07080c5cb",
      "4bb58946e12f4e9da36254271c79d320",
      "590f0d23be0746d99a811f7d74c2f470",
      "a0afd464d3fc409cb36ee6779ce1dd9e",
      "992bd6236dc747dc91bc8804a649ac3c",
      "a7c54687dece4f30ab1f91c2ca284d85",
      "178ea9b9e7d747bb8bfe294fa0c22b01",
      "dcbcd96129204014a1f7e42d70ac981d",
      "16bc48d3468c4c24999d1dcad3b45941",
      "868433b7e65d4dc9b8e74dc55bcdadcd",
      "f031e3affd984ba28987da66d0d2234e",
      "9875ec66f6e042fc9e6ad065a4b01a81",
      "c0b69df9a270439fb303acc4d27e6f59",
      "9ca48971a3154c0dbedc37441b07f52f",
      "09442076da0c44d39a17eea853b0d8b4",
      "271c6577916349e1a8252402d1ef4d4e",
      "2757958c3ee34100a80f85ebe9fc959e",
      "173b0fd46ba5453188568c0312fb0850",
      "c82a7542dd2f4c9faf7705b25b636f7a",
      "2caf88a18521463eb299544ff79baaa3",
      "f5a4353c6e5f450ab58764cba186e076",
      "05c1b027edb140ab93de3a451910088e",
      "0921533561b741eb9780851c81af7110",
      "a0e458eb6b2f43cc9627c6f8ccd6973c",
      "2a2981e51a7e45dcba4633ad1e000c7b",
      "067d7ed944284d78a52573a0101af232"
     ]
    },
    "id": "qwlhP0oniZIh",
    "outputId": "9c85cf85-daee-4d17-8e94-12a9f4bbaca4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26cb7b2101d648ad91b044311dfcfbd0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/65.4k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "12d980f73ef848e7bbffa96fac540b78"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f85f44c2f8bc4a7c948b803d59cc8ffe"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.53G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c33eb10fada64553bd95487f8688165e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1c6e740d8184e4a869a9bd16015bd9f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27360250220948dfa8d0b819fa9a90f9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11846300960d4ef797fc2a28aa00dddb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c996b4a8ff245749f7d98f01bc4516f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.70k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f1e8acdf17d41a7b24f7f2ec84f9044"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ef0ffdb03334aa29f980975e7dd91ea"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c55827dfff748b2ab1866abafc7e8ef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "992bd6236dc747dc91bc8804a649ac3c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "271c6577916349e1a8252402d1ef4d4e"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "class QwenVLM:\n",
    "    def __init__(self, model_name=\"Qwen/Qwen2.5-VL-3B-Instruct\", device=\"cuda\"):\n",
    "        # Load the model\n",
    "        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        # Load the processor\n",
    "        self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "        # Set device\n",
    "        self.device = device\n",
    "\n",
    "    def inference(self, prompt=\"Describe this image.\", image_path=None, max_new_tokens=128):\n",
    "        if image_path:\n",
    "            # Load image from local path\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "            # Construct messages in chat format with image\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            # Prepare text input\n",
    "            text = self.processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            # Prepare image/video inputs\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "            # Tokenize inputs\n",
    "            inputs = self.processor(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "\n",
    "        else:\n",
    "            # Text-only mode\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            text = self.processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            inputs = self.processor(\n",
    "                text=[text],\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "\n",
    "        # Generate output\n",
    "        generated_ids = self.model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "\n",
    "        # Decode output\n",
    "        output_text = self.processor.batch_decode(\n",
    "            generated_ids_trimmed,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )\n",
    "        return output_text\n",
    "\n",
    "vlm = QwenVLM()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2da7KNyCx7PX"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GpXHQLuwx8xc",
    "outputId": "9fb36e07-ff18-4ed5-b4ec-803708cb7591"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The image shows five colored shapes arranged in a row from left to right:\\n\\n1. A red square.\\n2. A green rectangle.\\n3. A yellow triangle.\\n4. A blue circle.\\n5. A black dot.\\n\\nEach shape is distinct in color and form, creating a simple yet colorful composition.']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "vlm.inference(image_path=image_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhjroR3_xozk"
   },
   "source": [
    "## LLM as a judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hzzp4h99L2-"
   },
   "source": [
    "```markdown\n",
    "**Guideline:** Your task:\n",
    "1. Design a prompt template that takes:\n",
    "   - The question.\n",
    "   - The model's answer.\n",
    "   - The ground truth answer.\n",
    "   And asks the LLM to determine if the model's answer implies the ground truth answer.\n",
    "2. Implement `judge(vlm, question, model_answer, ground_truth)` to:\n",
    "   - Construct and send your prompt to the VLM.\n",
    "   - Parse the VLM's \"Yes\"/\"No\" response into a binary `1` (correct) or `0` (incorrect).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2lXZa_Bn-CO0"
   },
   "outputs": [],
   "source": [
    "def judge(vlm, question, model_answer, ground_truth):\n",
    "    prompt = (\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Model's Answer: {model_answer}\\n\"\n",
    "        f\"Ground Truth: {ground_truth}\\n\"\n",
    "        \"Answer 'Yes' or 'No': Does the model's answer imply the ground truth answer?\"\n",
    "    )\n",
    "    # Send the prompt to the VLM and get the response\n",
    "    response = vlm.inference(prompt=prompt)\n",
    "    # Parse the response\n",
    "    answer = response[-1].strip().lower()\n",
    "    # Check if the answer is \"yes\" or \"no\"\n",
    "    if \"yes\" in answer:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VO8Q3Mkm9Pg_"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZKM-0zrj9R0e",
    "outputId": "90d5279b-3fbf-4dcf-8ce8-54b12185332a"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "\n",
    "judge(vlm, \"what is the shape of object?\",\"square\", \"I think shape of object is square\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSy_iM3lAGYU",
    "outputId": "9366a32c-e9b6-49d6-9d64-017eaed8253d"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# In this part should check the evalution procedure with negative examples\n",
    "judge(vlm, \"what is the shape of object?\", \"square\", \"the object is blue\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gY_kpZGxr4t"
   },
   "source": [
    "## zero-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFmrpDvM9L2-"
   },
   "source": [
    "```markdown\n",
    "**Guideline:** Implement the zero-shot evaluation loop:\n",
    "1. For each index `i` in the dataset:\n",
    "   - Use `vlm.inference(image_path=image_list[i], prompt=x[i])` to get the model's prediction.\n",
    "   - Store the prediction in `pred`.\n",
    "   - Call `judge(vlm, x[i], pred[-1], y[i])` to obtain a binary assessment of correctness.\n",
    "   - Append the judge's result to `true_pred`.\n",
    "2. Use `tqdm` to monitor progress.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hspLrfCXxxXQ",
    "outputId": "dd54c5e4-225f-4093-bf68-f94ec0e23f8f"
   },
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "pred = []\n",
    "true_pred = []\n",
    "\n",
    "# Loop over all samples in the dataset\n",
    "for i in tqdm(range(len(x))):\n",
    "    # Get model prediction for the i-th sample (image + question)\n",
    "    model_output = vlm.inference(image_path=image_list[i], prompt=x[i])\n",
    "    # Store the last output (the answer)\n",
    "    pred.append(model_output[-1])\n",
    "    # Print model's answer and ground truth answer\n",
    "    print()\n",
    "    print(f\"Model's answer: {pred[-1]}\")\n",
    "    print(f\"Ground truth: {y[i]}\")\n",
    "    # Use the judge function to assess correctness (1 = correct, 0 = incorrect)\n",
    "    true_pred.append(judge(vlm, x[i], pred[-1], y[i]))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vS1T1Sw-PTID",
    "outputId": "d6220c44-2a46-42ae-e425-2beec956d91e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Zero-shot accuracy: 55.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total accuracy\n",
    "accuracy = sum(true_pred) / len(true_pred)\n",
    "print(f\"Zero-shot accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79HZ-mfDuMId"
   },
   "source": [
    "## Build your own agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpvVxUpcQ8DB"
   },
   "source": [
    "### Classic agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crnCMQRs9L2-"
   },
   "source": [
    "```markdown\n",
    "**Guideline:**\n",
    "In the following you need use classic image processing techniques using opencv packages in addition to LLMs. Your agents should be classic approaches to process images and create a appropriate prompt for LLMs.\n",
    "\n",
    "In this section, given the constraint that agents must rely on classical approaches, your task is to implement a set of agents along with a verifier. The goal is to design a system (whether in a single execution or through a loop) that improves your VLM's ability to solve the problem.\n",
    "\n",
    "You're free to adjust the code structure or change the number of agents depending on what your solution requires.\n",
    "\n",
    "What's most important here is your creativity in designing the agents. For instance, you might start with an agent that detects the question type (e.g., whether it begins with \"what is\" or \"is the\"). Based on that, different agents can follow to handle specific tasks. For example, another agent might specialize in detecting object colors. By combining such agents, you can gradually build an effective system."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JO0sAm2WuVW9",
    "outputId": "5d115973-2e84-404d-d60a-024ab13c51be"
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "def agent1_shape(image_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Detects shapes in the image using color masks and contour approximation.\n",
    "    Returns a list of detected shape names (e.g., 'square', 'circle').\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    shapes = []\n",
    "    # Use all color masks to find all contours\n",
    "    COLOR_RANGES = {\n",
    "        'red': (([0, 120, 70], [10, 255, 255]), ([170, 120, 70], [180, 255, 255])),\n",
    "        'green': (([36, 100, 100], [86, 255, 255]),),\n",
    "        'blue': (([94, 80, 2], [126, 255, 255]),),\n",
    "        'yellow': (([22, 93, 0], [45, 255, 255]),),\n",
    "        'gray': (([0, 0, 40], [180, 50, 220]),),\n",
    "        'black': (([0, 0, 0], [180, 255, 30]),)\n",
    "    }\n",
    "    for hsv_ranges in COLOR_RANGES.values():\n",
    "        combined_mask = cv2.inRange(hsv_img, np.array(hsv_ranges[0][0]), np.array(hsv_ranges[0][1]))\n",
    "        if len(hsv_ranges) > 1:\n",
    "            for i in range(1, len(hsv_ranges)):\n",
    "                mask_part = cv2.inRange(hsv_img, np.array(hsv_ranges[i][0]), np.array(hsv_ranges[i][1]))\n",
    "                combined_mask = cv2.bitwise_or(combined_mask, mask_part)\n",
    "        contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for contour in contours:\n",
    "            if cv2.contourArea(contour) < 100:\n",
    "                continue\n",
    "            peri = cv2.arcLength(contour, True)\n",
    "            approx = cv2.approxPolyDP(contour, 0.04 * peri, True)\n",
    "            shape = \"square\" if len(approx) == 4 else \"circle\"\n",
    "            shapes.append(shape)\n",
    "    return shapes\n",
    "\n",
    "def agent2_color(image_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Detects colors of objects in the image using HSV color masks.\n",
    "    Returns a list of detected color names.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    colors_found = []\n",
    "    COLOR_RANGES = {\n",
    "        'red': (([0, 120, 70], [10, 255, 255]), ([170, 120, 70], [180, 255, 255])),\n",
    "        'green': (([36, 100, 100], [86, 255, 255]),),\n",
    "        'blue': (([94, 80, 2], [126, 255, 255]),),\n",
    "        'yellow': (([22, 93, 0], [45, 255, 255]),),\n",
    "        'gray': (([0, 0, 40], [180, 50, 220]),),\n",
    "        'black': (([0, 0, 0], [180, 255, 30]),)\n",
    "    }\n",
    "    for color, hsv_ranges in COLOR_RANGES.items():\n",
    "        combined_mask = cv2.inRange(hsv_img, np.array(hsv_ranges[0][0]), np.array(hsv_ranges[0][1]))\n",
    "        if len(hsv_ranges) > 1:\n",
    "            for i in range(1, len(hsv_ranges)):\n",
    "                mask_part = cv2.inRange(hsv_img, np.array(hsv_ranges[i][0]), np.array(hsv_ranges[i][1]))\n",
    "                combined_mask = cv2.bitwise_or(combined_mask, mask_part)\n",
    "        contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for contour in contours:\n",
    "            if cv2.contourArea(contour) < 100:\n",
    "                continue\n",
    "            colors_found.append(color)\n",
    "    return colors_found\n",
    "\n",
    "def agent3_centroid(image_path: str) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Calculates centroids of detected objects in the image using color masks and contour moments.\n",
    "    Returns a list of (x, y) centroid coordinates.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    centroids = []\n",
    "    COLOR_RANGES = {\n",
    "        'red': (([0, 120, 70], [10, 255, 255]), ([170, 120, 70], [180, 255, 255])),\n",
    "        'green': (([36, 100, 100], [86, 255, 255]),),\n",
    "        'blue': (([94, 80, 2], [126, 255, 255]),),\n",
    "        'yellow': (([22, 93, 0], [45, 255, 255]),),\n",
    "        'gray': (([0, 0, 40], [180, 50, 220]),),\n",
    "        'black': (([0, 0, 0], [180, 255, 30]),)\n",
    "    }\n",
    "    for hsv_ranges in COLOR_RANGES.values():\n",
    "        combined_mask = cv2.inRange(hsv_img, np.array(hsv_ranges[0][0]), np.array(hsv_ranges[0][1]))\n",
    "        if len(hsv_ranges) > 1:\n",
    "            for i in range(1, len(hsv_ranges)):\n",
    "                mask_part = cv2.inRange(hsv_img, np.array(hsv_ranges[i][0]), np.array(hsv_ranges[i][1]))\n",
    "                combined_mask = cv2.bitwise_or(combined_mask, mask_part)\n",
    "        contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for contour in contours:\n",
    "            if cv2.contourArea(contour) < 100:\n",
    "                continue\n",
    "            M = cv2.moments(contour)\n",
    "            if M[\"m00\"] == 0:\n",
    "                continue\n",
    "            cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "            centroids.append((cx, cy))\n",
    "    return centroids\n",
    "\n",
    "def query_vlm(prompt: str, image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends the prompt and image to the VLM and returns the first response.\n",
    "    \"\"\"\n",
    "    return vlm.inference(image_path=image_path, prompt=prompt)[0]\n",
    "\n",
    "def generate_prompt(prompt: str, image_path: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Generates a new prompt by extracting shapes, colors, and centroids from the image,\n",
    "    and appending this information to the original prompt.\n",
    "    Returns the new prompt and image path.\n",
    "    \"\"\"\n",
    "    shapes = agent1_shape(image_path)\n",
    "    colors = agent2_color(image_path)\n",
    "    centroids = agent3_centroid(image_path)\n",
    "    detected_objects = list(zip(shapes, colors, centroids))\n",
    "    if not detected_objects:\n",
    "        return prompt, image_path\n",
    "    prompt = (\n",
    "        f\"Based on the following information about the objects in the image: {detected_objects}. \"\n",
    "        f\"Please answer this question: '{prompt}'\"\n",
    "    )\n",
    "    return prompt, image_path\n",
    "\n",
    "def verify_response(prompt: str, image_path: str, response: str) -> bool:\n",
    "    \"\"\"\n",
    "    Verifies if the response is appropriate for the question type (color, shape, direction, yes/no).\n",
    "    Returns True if the response matches expected keywords, otherwise False.\n",
    "    \"\"\"\n",
    "    if \"color\" in prompt.lower():\n",
    "        colors = [\"red\", \"blue\", \"green\", \"yellow\", \"black\", \"gray\"]\n",
    "        return any(color in response.lower() for color in colors)\n",
    "    elif \"shape\" in prompt.lower():\n",
    "        shapes = [\"circle\", \"square\", \"triangle\", \"rectangle\"]\n",
    "        return any(shape in response.lower() for shape in shapes)\n",
    "    elif any(direction in prompt.lower() for direction in [\"left\", \"right\", \"top\", \"bottom\"]):\n",
    "        directions = [\"left\", \"right\", \"top\", \"bottom\"]\n",
    "        return any(direction in response.lower() for direction in directions)\n",
    "\n",
    "    return True\n",
    "\n",
    "def agent_loop(question: str, image_path: str, max_retries=1):\n",
    "    \"\"\"\n",
    "    Main agent loop that generates a prompt, queries the VLM, and verifies the response.\n",
    "    Retries up to max_retries times if the response is not valid.\n",
    "    Returns the final response.\n",
    "    \"\"\"\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        prompt, image_path = generate_prompt(question, image_path)\n",
    "        response = query_vlm(prompt=prompt, image_path=image_path)\n",
    "        if verify_response(prompt, image_path, response):\n",
    "            return response\n",
    "        retries += 1\n",
    "    return response\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "pred = []\n",
    "true_pred = []\n",
    "\n",
    "for i in tqdm(range(len(x))):\n",
    "    pred.append(agent_loop(image_path=image_list[i], question=x[i]))\n",
    "    print()\n",
    "    print(f\"Model's answer: {pred[-1]}\")\n",
    "    print(f\"Ground truth: {y[i]}\")\n",
    "    true_pred.append(judge(vlm, x[i], pred[-1], y[i]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YqtB0qgEzuRW",
    "outputId": "445e8bf8-03bc-4b19-ade0-338ff27c67dc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classic agents accuracy: 65.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total accuracy\n",
    "accuracy = sum(true_pred) / len(true_pred)\n",
    "print(f\"Classic agents accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6HMz_5v9L2_"
   },
   "source": [
    "### Ablation Study\n",
    "\n",
    "```markdown\n",
    "In this part, you need to test the impact of each agent by removing them one by one to assess how much each contributes to the system’s accuracy. Your goal is to optimize the system and identify the most effective configuration. You should also include a written analysis explaining the role of each agent and justify why it's necessary based on its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HXVYN0um9L2_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e990af25-7952-45d3-aad1-557b8bc449eb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No agent1_shape: 100%|██████████| 100/100 [03:06<00:00,  1.87s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy without agent1_shape: 48.00%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No agent2_color: 100%|██████████| 100/100 [03:16<00:00,  1.97s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy without agent2_color: 61.00%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No agent3_centroid: 100%|██████████| 100/100 [02:50<00:00,  1.71s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy without agent3_centroid: 64.00%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Ablation: Remove agent1_shape (shapes)\n",
    "def generate_prompt_no_shape(prompt: str, image_path: str) -> tuple:\n",
    "    colors = agent2_color(image_path)\n",
    "    centroids = agent3_centroid(image_path)\n",
    "    detected_objects = list(zip(colors, centroids))\n",
    "    if not detected_objects:\n",
    "        return prompt, image_path\n",
    "    prompt = (\n",
    "        f\"Based on the following information about the objects in the image: {detected_objects}. \"\n",
    "        f\"Please answer this question: '{prompt}'\"\n",
    "    )\n",
    "    return prompt, image_path\n",
    "\n",
    "# Ablation: Remove agent2_color (colors)\n",
    "def generate_prompt_no_color(prompt: str, image_path: str) -> tuple:\n",
    "    shapes = agent1_shape(image_path)\n",
    "    centroids = agent3_centroid(image_path)\n",
    "    detected_objects = list(zip(shapes, centroids))\n",
    "    if not detected_objects:\n",
    "        return prompt, image_path\n",
    "    prompt = (\n",
    "        f\"Based on the following information about the objects in the image: {detected_objects}. \"\n",
    "        f\"Please answer this question: '{prompt}'\"\n",
    "    )\n",
    "    return prompt, image_path\n",
    "\n",
    "# Ablation: Remove agent3_centroid (centroids)\n",
    "def generate_prompt_no_centroid(prompt: str, image_path: str) -> tuple:\n",
    "    shapes = agent1_shape(image_path)\n",
    "    colors = agent2_color(image_path)\n",
    "    detected_objects = list(zip(shapes, colors))\n",
    "    if not detected_objects:\n",
    "        return prompt, image_path\n",
    "    prompt = (\n",
    "        f\"Based on the following information about the objects in the image: {detected_objects}. \"\n",
    "        f\"Please answer this question: '{prompt}'\"\n",
    "    )\n",
    "    return prompt, image_path\n",
    "\n",
    "def agent_loop_ablation(question, image_path, generate_prompt_func, max_retries=1):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        prompt, image_path = generate_prompt_func(question, image_path)\n",
    "        response = query_vlm(prompt=prompt, image_path=image_path)\n",
    "        if verify_response(prompt, image_path, response):\n",
    "            return response\n",
    "        retries += 1\n",
    "    return response\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ablation 1: Without agent1_shape\n",
    "pred1, true_pred1 = [], []\n",
    "for i in tqdm(range(len(x)), desc=\"No agent1_shape\"):\n",
    "    pred1.append(agent_loop_ablation(x[i], image_list[i], generate_prompt_no_shape))\n",
    "    true_pred1.append(judge(vlm, x[i], pred1[-1], y[i]))\n",
    "acc1 = sum(true_pred1) / len(true_pred1)\n",
    "print(f\"Accuracy without agent1_shape: {acc1 * 100:.2f}%\")\n",
    "\n",
    "# Ablation 2: Without agent2_color\n",
    "pred2, true_pred2 = [], []\n",
    "for i in tqdm(range(len(x)), desc=\"No agent2_color\"):\n",
    "    pred2.append(agent_loop_ablation(x[i], image_list[i], generate_prompt_no_color))\n",
    "    true_pred2.append(judge(vlm, x[i], pred2[-1], y[i]))\n",
    "acc2 = sum(true_pred2) / len(true_pred2)\n",
    "print(f\"Accuracy without agent2_color: {acc2 * 100:.2f}%\")\n",
    "\n",
    "# Ablation 3: Without agent3_centroid\n",
    "pred3, true_pred3 = [], []\n",
    "for i in tqdm(range(len(x)), desc=\"No agent3_centroid\"):\n",
    "    pred3.append(agent_loop_ablation(x[i], image_list[i], generate_prompt_no_centroid))\n",
    "    true_pred3.append(judge(vlm, x[i], pred3[-1], y[i]))\n",
    "acc3 = sum(true_pred3) / len(true_pred3)\n",
    "print(f\"Accuracy without agent3_centroid: {acc3 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjYRFPKFhJpY"
   },
   "source": [
    "### DL Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2Yj64DM9L2_"
   },
   "source": [
    "```markdown\n",
    "**Guideline:** In the Deep Learning Agents section, you will develop enhanced agents using LLM techniques. For each agent (`agent1`, `agent2`, `agent3`):\n",
    "1. Modify the agent function to:\n",
    "   - Here, you need to using Deep Learning techniques to build agents beside LLM.\n",
    "   - Generate an intermediate reasoning step or sub-prompt via the VLM.\n",
    "   - Use that reasoning to produce a final answer tuple `(intermediate, final)`.\n",
    "2. Ensure that `agent_loop` integrates these steps to iteratively refine answers based on VLM feedback.\n",
    "\n",
    "In this section, given the constraint that agents must rely on Deep Learning approaches, your task is to implement a set of agents along with a verifier. The goal is to design a system (whether in a single execution or through a loop) that improves your VLM's ability to solve the problem.\n",
    "\n",
    "You're free to adjust the code structure or change the number of agents depending on what your solution requires.\n",
    "\n",
    "What's most important here is your creativity in designing the agents. For instance, you might start with an agent that detects the question type (e.g., whether it begins with \"what is\" or \"is the\"). Based on that, different agents can follow to handle specific tasks. For example, another agent might specialize in instance segmentation (e.g. Segment Anything Model (SAM) model). By combining such agents, you can gradually build an effective system.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install git+https://github.com/facebookresearch/segment-anything.git"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wm-RYAcqJY3v",
    "outputId": "e9b24cce-f356-49fe-b325-7224a1b51a8c"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
      "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-1y23je3o\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-1y23je3o\n",
      "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Building wheels for collected packages: segment_anything\n",
      "  Building wheel for segment_anything (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for segment_anything: filename=segment_anything-1.0-py3-none-any.whl size=36592 sha256=7d29c0ae9be5cc3b5be84c31f5f2fe62eaef13354d7781d8a59bc0ed64a31958\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hlq5drp5/wheels/15/d7/bd/05f5f23b7dcbe70cbc6783b06f12143b0cf1a5da5c7b52dcc5\n",
      "Successfully built segment_anything\n",
      "Installing collected packages: segment_anything\n",
      "Successfully installed segment_anything-1.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wh00or9DJbZX",
    "outputId": "feea5606-7e18-4c2e-9740-b9ff8e10f5ba"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2025-06-12 15:54:33--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.37.90, 13.35.37.123, 13.35.37.84, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.37.90|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
      "Saving to: ‘sam_vit_h_4b8939.pth’\n",
      "\n",
      "sam_vit_h_4b8939.pt 100%[===================>]   2.39G   163MB/s    in 19s     \n",
      "\n",
      "2025-06-12 15:54:53 (127 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tn8RuR-jWMql",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1ec1852b-d467-43a7-ec93-03e3879205ac"
   },
   "source": [
    "import torch\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "def query_vlm(prompt: str, image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends the prompt and image to the VLM and returns the first response.\n",
    "    \"\"\"\n",
    "    return vlm.inference(image_path=image_path, prompt=prompt)[0]\n",
    "\n",
    "# Initialize SAM\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "mask_generator = SamAutomaticMaskGenerator(model=sam, points_per_side=8, min_mask_region_area=100)\n",
    "\n",
    "def agent1_segment(image_path: str):\n",
    "    \"\"\"Segment objects using SAM and return masks and bounding boxes.\"\"\"\n",
    "    image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "    masks = mask_generator.generate(image)\n",
    "    # Each mask: dict with 'segmentation', 'bbox', etc.\n",
    "    return masks\n",
    "\n",
    "def agent2_color(image_path: str, masks):\n",
    "    \"\"\"Classify color of each segment using VLM.\"\"\"\n",
    "    colors = []\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    for idx, mask in enumerate(masks):\n",
    "        bbox = mask['bbox']\n",
    "        crop = image.crop((bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]))\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".png\", delete=True) as tmp:\n",
    "            crop.save(tmp.name)\n",
    "            prompt = \"What is the color of the object in this image? Answer with one of the following: red, blue, green, yellow, black, gray.\"\n",
    "            color = query_vlm(prompt=prompt, image_path=tmp.name)\n",
    "            colors.append(color.strip())\n",
    "    return colors\n",
    "\n",
    "def agent3_shape(image_path: str, masks):\n",
    "    \"\"\"Classify shape of each segment using VLM.\"\"\"\n",
    "    shapes = []\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    for idx, mask in enumerate(masks):\n",
    "        bbox = mask['bbox']\n",
    "        crop = image.crop((bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]))\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".png\", delete=True) as tmp:\n",
    "            crop.save(tmp.name)\n",
    "            prompt = \"What is the shape of the object in this image? Answer with square or circle.\"\n",
    "            shape = query_vlm(prompt=prompt, image_path=tmp.name)\n",
    "            shapes.append(shape.strip())\n",
    "    return shapes\n",
    "\n",
    "def generate_prompt(prompt: str, image_path: str):\n",
    "    \"\"\"Generate prompt using detected objects' color and shape.\"\"\"\n",
    "    masks = agent1_segment(image_path)\n",
    "    if not masks:\n",
    "        return prompt, image_path\n",
    "    colors = agent2_color(image_path, masks)\n",
    "    shapes = agent3_shape(image_path, masks)\n",
    "    detected_objects = list(zip(shapes, colors))\n",
    "    prompt_full = (\n",
    "        f\"Based on the following detected objects (shape, color): {detected_objects}. \"\n",
    "        f\"Please answer this question: '{prompt}'\"\n",
    "    )\n",
    "    return prompt_full, image_path\n",
    "\n",
    "def verify_response(prompt: str, image_path: str, response: str) -> bool:\n",
    "    \"\"\"Simple keyword-based verification.\"\"\"\n",
    "    if \"color\" in prompt.lower():\n",
    "        colors = [\"red\", \"blue\", \"green\", \"yellow\", \"black\", \"gray\"]\n",
    "        return any(color in response.lower() for color in colors)\n",
    "    elif \"shape\" in prompt.lower():\n",
    "        shapes = [\"circle\", \"square\", \"triangle\", \"rectangle\"]\n",
    "        return any(shape in response.lower() for shape in shapes)\n",
    "    elif any(direction in prompt.lower() for direction in [\"left\", \"right\", \"top\", \"bottom\"]):\n",
    "        directions = [\"left\", \"right\", \"top\", \"bottom\"]\n",
    "        return any(direction in response.lower() for direction in directions)\n",
    "\n",
    "    return True\n",
    "\n",
    "def agent_loop(question: str, image_path: str, max_retries=1):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        prompt, image_path = generate_prompt(question, image_path)\n",
    "        response = vlm.inference(prompt=prompt, image_path=image_path)[0]\n",
    "        if verify_response(prompt, image_path, response):\n",
    "            return response\n",
    "        retries += 1\n",
    "    return response\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "pred = []\n",
    "true_pred = []\n",
    "\n",
    "for i in tqdm(range(len(x))):\n",
    "    pred.append(agent_loop(image_path=image_list[i], question=x[i]))\n",
    "    print()\n",
    "    print(f\"Model's answer: {pred[-1]}\")\n",
    "    print(f\"Ground truth: {y[i]}\")\n",
    "    true_pred.append(judge(vlm, x[i], pred[-1], y[i]))\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "wMimdx9E9L2_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8470dc34-8623-479d-975b-b84552ab5716"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DL agents accuracy: 58.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total accuracy\n",
    "accuracy = sum(true_pred) / len(true_pred)\n",
    "print(f\"DL agents accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8X1lk329L2_"
   },
   "source": [
    "### Ablation Study\n",
    "\n",
    "```markdown\n",
    "In this part, you need to test the impact of each agent by removing them one by one to assess how much each contributes to the system’s accuracy. Your goal is to optimize the system and identify the most effective configuration. You should also include a written analysis explaining the role of each agent and justify why it's necessary based on its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "dsD3qt0JSW70",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "260f070f-54b8-4c0b-816b-5d29538e329a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No agent2_color: 100%|██████████| 100/100 [10:32<00:00,  6.33s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy without agent2_color: 62.00%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No agent3_shape: 100%|██████████| 100/100 [11:23<00:00,  6.83s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy without agent3_shape: 54.00%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Ablation 1: Without agent1_segment (no segmentation, just use original prompt)\n",
    "# This one is pointless\n",
    "# def generate_prompt_no_segment(prompt: str, image_path: str):\n",
    "#     # No segmentation, just use the original prompt\n",
    "#     return prompt, image_path\n",
    "\n",
    "# Ablation 2: Without agent2_color (no color, only shape)\n",
    "def generate_prompt_no_color(prompt: str, image_path: str):\n",
    "    masks = agent1_segment(image_path)\n",
    "    if not masks:\n",
    "        return prompt, image_path\n",
    "    shapes = agent3_shape(image_path, masks)\n",
    "    detected_objects = [(shape,) for shape in shapes]\n",
    "    prompt_full = (\n",
    "        f\"Based on the following detected objects (shape): {detected_objects}. \"\n",
    "        f\"Please answer this question: '{prompt}'\"\n",
    "    )\n",
    "    return prompt_full, image_path\n",
    "\n",
    "# Ablation 3: Without agent3_shape (no shape, only color)\n",
    "def generate_prompt_no_shape(prompt: str, image_path: str):\n",
    "    masks = agent1_segment(image_path)\n",
    "    if not masks:\n",
    "        return prompt, image_path\n",
    "    colors = agent2_color(image_path, masks)\n",
    "    detected_objects = [(color,) for color in colors]\n",
    "    prompt_full = (\n",
    "        f\"Based on the following detected objects (color): {detected_objects}. \"\n",
    "        f\"Please answer this question: '{prompt}'\"\n",
    "    )\n",
    "    return prompt_full, image_path\n",
    "\n",
    "def agent_loop_ablation(question: str, image_path: str, generate_prompt_func, max_retries=1):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        prompt, image_path = generate_prompt_func(question, image_path)\n",
    "        response = query_vlm(prompt=prompt, image_path=image_path)\n",
    "        if verify_response(prompt, image_path, response):\n",
    "            return response\n",
    "        retries += 1\n",
    "    return response\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ablation 1: No segmentation\n",
    "# pred_no_segment, true_pred_no_segment = [], []\n",
    "# for i in tqdm(range(len(x)), desc=\"No agent1_segment\"):\n",
    "#     pred_no_segment.append(agent_loop_ablation(x[i], image_list[i], generate_prompt_no_segment))\n",
    "#     true_pred_no_segment.append(judge(vlm, x[i], pred_no_segment[-1], y[i]))\n",
    "# acc_no_segment = sum(true_pred_no_segment) / len(true_pred_no_segment)\n",
    "# print(f\"Accuracy without agent1_segment (SAM): {acc_no_segment * 100:.2f}%\")\n",
    "\n",
    "# Ablation 2: No color\n",
    "pred_no_color, true_pred_no_color = [], []\n",
    "for i in tqdm(range(len(x)), desc=\"No agent2_color\"):\n",
    "    pred_no_color.append(agent_loop_ablation(x[i], image_list[i], generate_prompt_no_color))\n",
    "    true_pred_no_color.append(judge(vlm, x[i], pred_no_color[-1], y[i]))\n",
    "acc_no_color = sum(true_pred_no_color) / len(true_pred_no_color)\n",
    "print(f\"Accuracy without agent2_color: {acc_no_color * 100:.2f}%\")\n",
    "\n",
    "# Ablation 3: No shape\n",
    "pred_no_shape, true_pred_no_shape = [], []\n",
    "for i in tqdm(range(len(x)), desc=\"No agent3_shape\"):\n",
    "    pred_no_shape.append(agent_loop_ablation(x[i], image_list[i], generate_prompt_no_shape))\n",
    "    true_pred_no_shape.append(judge(vlm, x[i], pred_no_shape[-1], y[i]))\n",
    "acc_no_shape = sum(true_pred_no_shape) / len(true_pred_no_shape)\n",
    "print(f\"Accuracy without agent3_shape: {acc_no_shape * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1FJCF1T9L3G"
   },
   "source": [
    "## 🎯 Final Educational Objectives\n",
    "\n",
    "In the final step, you should present your analysis by comparing the zero-shot approach with the two agent-driven systems you've built. Your comparison should cover not only the final accuracy but also other factors like execution time.\n",
    "\n",
    "\n",
    "\\begin{array}{lcc}\n",
    "\\textbf{Method} & \\textbf{Accuracy} & \\textbf{Time (100\\ samples)} \\\\\n",
    "\\hline\n",
    "\\text{Zero-shot} & 55\\% & 2{:}23 \\\\\n",
    "\\text{Classic Agents} & 65\\% & 3{:}34 \\\\\n",
    "\\text{DL Agents} & 58\\% & 15{:}37 \\\\\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "Classic Agents Ablation\n",
    "- No agent1_shape: 48% (3:06)\n",
    "- No agent2_color: 61% (3:16)\n",
    "- No agent3_centroid: 64% (2:50)\n",
    "\n",
    "DL Agents Ablation\n",
    "- No agent2_color: 62% (10:32)\n",
    "- No agent3_shape: 54% (11:23)\n",
    "- No agent1_segment: Not meaningful (segmentation is essential)\n",
    "\n",
    "Accuracy: Classic agents outperform both zero-shot and DL agents, achieving the highest accuracy (65%). DL agents show only a modest improvement over zero-shot (58% vs. 55%), despite much longer execution time.\n",
    "\n",
    "Execution Time: Zero-shot is fastest. Classic agents add moderate overhead for image processing. DL agents are significantly slower due to segmentation and multiple VLM calls.\n",
    "\n",
    "Ablation Insights: In classic agents, removing shape detection (agent1_shape) causes the largest drop in accuracy, showing its critical role. Color detection (agent2_color) is also important, but less so than shape. Centroid information (agent3_centroid) has the least impact.\n",
    "\n",
    "For DL agents, the shape detection component contributes positively to performance, while the color detection agent does not help and even degrade results. This is likely due to the VLM's limited ability to accurately perceive colors. Incorporating a dedicated, task-specific deep learning model for color recognition could potentially improve overall performance.\n",
    "Segmentation is essential for DL agents. without it, the system reverts to zero-shot performance. So I didnt include it in the ablation study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYifyWxd9L3G"
   },
   "source": [
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- ✅ Understand how LLMs can act as intelligent agents.\n",
    "- ✅ Apply multimodal models to structured reasoning tasks.\n",
    "- ✅ Evaluate the outputs of models using self-judging agents.\n",
    "- ✅ Build your **own agent pipeline** and apply intermediate reasoning steps.\n",
    "- ✅ Reflect on how prompting and architecture influence results.\n",
    "\n",
    "---\n",
    "\n",
    "🎓 *This notebook equips you with essential skills to work on cutting-edge AI systems that combine vision, language, and planning.*\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
